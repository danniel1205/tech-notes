{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build local RAG\n",
    "\n",
    "In this notebook, I am going to walkthrough what I have done to build a simple local RAG by leveraging open source LLM and embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install ollama\n",
    "\n",
    "Follow this [link](https://ollama.com/) to install ollama on you local machine.\n",
    "\n",
    "## (Optional) Install Conda\n",
    "\n",
    "Follow this [link](https://docs.conda.io/projects/conda/en/latest/index.html) to install Conda which is for package and python environment management. Otherwise, you have to manually `pip install` the missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Only needed if you use Conda) Create a local python environment and automatically install all required packages\n",
    "\n",
    "!conda env create -f environment.yaml\n",
    "!conda activte local-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ollama to pull the model you plan to use\n",
    "\n",
    "!ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LLM without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Autopilot is a system or technology that enables an aircraft, vehicle, or other device to navigate and control itself automatically, without human intervention. The term \"autopilot\" was first coined in the early 20th century for use in aviation.\\n\\nIn modern times, the concept of autopilot has expanded beyond just aircraft and vehicles to include various applications across multiple industries. Here are some examples:\\n\\n1. **Aviation Autopilot**: In aircraft, autopilot systems use sensors, computers, and software to control flight trajectory, altitude, speed, and navigation.\\n2. **Automotive Autopilot**: In cars and trucks, advanced driver-assistance systems (ADAS) like lane-keeping assist, adaptive cruise control, and automatic emergency braking can be considered types of autopilot.\\n3. **Marine Autopilot**: Similar to aviation autopilot, marine autopilot systems help navigate boats and yachts by automatically controlling speed, direction, and depth.\\n4. **GPS Autopilot**: Global Positioning System (GPS) technology uses satellite signals to provide navigation data, which can be used to enable autonomous systems like drones or self-driving cars.\\n\\nAutopilot systems typically rely on a combination of sensors, software, and algorithms to achieve their goals. Some common components include:\\n\\n* GPS and inertial measurement units (IMUs)\\n* Radar and lidar sensors\\n* Cameras and computer vision\\n* Machine learning models and artificial intelligence (AI)\\n\\nWhile autopilot technology has made significant progress in recent years, it\\'s essential to note that true autonomy in complex environments still requires human oversight and intervention. The development of autonomous systems continues to push the boundaries of innovation, safety, and efficiency across various industries.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "model = OllamaLLM(model=MODEL)\n",
    "model.invoke(\"What is autopilot?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LLM by providing a simple context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Daniel.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you cannot answer the questinon, reply \"I do not know\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\n",
    "    \"context\": \"The name I was given when I was born was Daniel\",\n",
    "    \"question\": \"What is my name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use loadl PDF as the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='Autopilot: workload autoscaling at Google\\nKrzysztof Rzadca\\nGoogle + Univ. of Warsaw\\nkmrz@google.com\\nPawel Findeisen\\nGoogle\\npafinde@google.com\\nJacek Swiderski\\nGoogle\\njswiderski@google.com\\nPrzemyslaw Zych\\nGoogle\\npzych@google.com\\nPrzemyslaw Broniek\\nGoogle\\nbroniek@google.com\\nJarek Kusmierek\\nGoogle\\njdk@google.com\\nPawel Nowak\\nGoogle\\npawelnow@google.com\\nBeata Strack\\nGoogle\\nbstrack@google.com\\nPiotr Witusowski\\nGoogle\\nwitus@google.com\\nSteven Hand\\nGoogle\\nsthand@google.com\\nJohn Wilkes\\nGoogle\\njohnwilkes@google.com\\nAbstract\\nIn many public and private Cloud systems, users need to\\nspecify a limit for the amount of resources (CPU cores and\\nRAM) to provision for their workloads. A job that exceeds\\nits limits might be throttled or killed, resulting in delaying or\\ndropping end-user requests, so human operators naturally\\nerr on the side of caution and request a larger limit than the\\njob needs. At scale, this results in massive aggregate resource\\nwastage.\\nTo address this, Google uses Autopilot to configure re-\\nsources automatically, adjusting both the number of concur-\\nrent tasks in a job (horizontal scaling) and the CPU/memory\\nlimits for individual tasks (vertical scaling). Autopilot walks\\nthe same fine line as human operators: its primary goal is\\nto reduce slack – the difference between the limit and the\\nactual resource usage – while minimizing the risk that a task\\nis killed with an out-of-memory (OOM) error or its perfor-\\nmance degraded because of CPU throttling. Autopilot uses\\nmachine learning algorithms applied to historical data about\\nprior executions of a job, plus a set of finely-tuned heuristics,\\nto walk this line. In practice, Autopiloted jobs have a slack\\nof just 23%, compared with 46% for manually-managed jobs.\\nAdditionally, Autopilot reduces the number of jobs severely\\nimpacted by OOMs by a factor of 10.\\nDespite its advantages, ensuring that Autopilot was widely\\nadopted took significant effort, including making potential\\nrecommendations easily visible to customers who had yet\\nto opt in, automatically migrating certain categories of jobs,\\nand adding support for custom recommenders. At the time\\nPermission to make digital or hard copies of part or all of this work for\\npersonal or classroom use is granted without fee provided that copies are\\nnot made or distributed for profit or commercial advantage and that copies\\nbear this notice and the full citation on the first page. Copyrights for third-\\nparty components of this work must be honored. For all other uses, contact\\nthe owner/author(s).\\nEuroSys ’20, April 27–30, 2020, Heraklion, Greece\\n© 2020 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-6882-7/20/04.\\nhttps://doi.org/10.1145/3342195.3387524\\nMachineMachineMachine\\nRecommenders\\nAutopilot \\nservice Actuator Borgmaster\\nresource\\nusage log\\nMoving \\nwindow\\nML\\ncustom\\ntasktasktask Borglet\\ntask limits,\\nstart/stop\\ntask counts, \\ntask limits\\nrecommended \\nlimits\\ncgroup limits,\\nstart/stopusage data\\nFigure 1. Autopilot dataflow diagram.\\nof writing, Autopiloted jobs account for over 48% of Google’s\\nfleet-wide resource usage.\\nACM Reference Format:\\nKrzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw\\nZych, Przemyslaw Broniek, Jarek Kusmierek, Pawel Nowak, Beata\\nStrack, Piotr Witusowski, Steven Hand, and John Wilkes. 2020.\\nAutopilot: workload autoscaling at Google. In Fifteenth European\\nConference on Computer Systems (EuroSys ’20), April 27–30, 2020,\\nHeraklion, Greece. ACM, New York, NY, USA, 16 pages. https://doi.\\norg/10.1145/3342195.3387524\\n1 Introduction\\nMany types of public and private Cloud systems require\\ntheir users to declare how many instances their workload\\nwill need during execution, and the resources needed for\\neach: in public cloud platforms, users need to choose the\\ntype and the number of virtual machines (VMs) they will\\nrent; in a Kubernetes cluster, users set the number of pod\\nreplicas and resource limits for individual pods; in Google,\\nwe ask users to specify the number of containers they need'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='rent; in a Kubernetes cluster, users set the number of pod\\nreplicas and resource limits for individual pods; in Google,\\nwe ask users to specify the number of containers they need\\nand the resource limits for each. Such limits make cloud\\ncomputing possible, by enabling the Cloud infrastructure to\\nprovide adequate performance isolation.\\nBut limits are (mostly) a nuisance to the user. It is hard to\\nestimate how many resources a job needs to run optimally:\\nthe right combination of CPU power, memory, and the num-\\nber of concurrently running replicas. Load tests can help'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\nfind an initial estimate, but these recommendations will be-\\ncome stale as resource needs change over time because many\\nend-user serving jobs have diurnal or weekly load patterns,\\nand traffic changes across longer time scales as a service\\nbecomes more or less popular. Finally, the resources needed\\nto handle a given load vary with new features, optimizations\\nand updates of the underlying software or hardware stack.\\nExceeding the requested resources can result in poor per-\\nformance if the CPU is capped, or cause a task to be killed\\nbecause it runs out of memory (an OOM). Neither is good.\\nAs a result, a rational user will deliberately overestimate\\nthe resources their jobs need, resulting in poor utilization\\nof physical resources. One analysis [ 26] of a month-long\\ntrace of jobs executed at one of Google’s clusters [27] shows\\n50% average memory utilization; another analysis [23] of Al-\\nibaba’s YARN cluster showed tasks’ peak memory utilization\\nnever exceeding 80%.\\nIn response to the difficulties in configuring resources, a\\ncommon pattern is to adopt a horizontal autoscaler, which\\nscales a job by adding or removing replicas in response to\\nchanges in the end-user traffic, or the average CPU utiliza-\\ntion. All major cloud providers (AWS, Azure and GCP) pro-\\nvide a horizontal autoscaling function; it is also available in\\nsome Cloud middleware, such as Kubernetes. A less common\\npattern is to use vertical autoscaling to tune the amount of\\nresources available to each replica. The two techniques can\\nalso be combined.\\nAutopilot is the primary autoscaler that Google uses on\\nits internal cloud. It provides both horizontal and vertical au-\\ntoscaling. This paper focuses on Autopilot’s vertical scaling\\nof memory, since this is less commonly reported. The paper:\\n•Describes Autopilot, and the two main algorithms\\nit uses for vertical autoscaling: the first relies on an\\nexponentially-smoothed sliding window over historic\\nusage; the other is a meta-algorithm based on ideas\\nborrowed from reinforcement learning, which runs\\nmany variants of the sliding window algorithm and\\nchooses the one that would have historically resulted\\nin the best performance for each job;\\n•Evaluates the effectiveness of Autopilot’s algorithms\\non representative samples of Google’s workload; and\\n•Discusses the steps we took to have Autopilot widely\\nadopted across our fleet.\\n2 Managing cloud resources with Borg\\nAutopilot’s goals and constraints follow from Google’s Borg\\ninfrastructure, and it is tuned for Google’s workload. We\\nprovide a brief overview of both here: for more details about\\nBorg see [34], and for more detailed information about the\\nworkload see [26, 27, 31, 35].\\n2.1 Machines, jobs and tasks\\nThe Google compute infrastructure is made up of many clus-\\nters, spread in multiple geographical locations. A median\\ncluster has roughly 10 000 physical machines, and runs many\\ndifferent kinds of workloads simultaneously. A single physi-\\ncal machine might simultaneously compute a memory- and\\nCPU-heavy batch computation, store and serve queries for a\\nslice of a memory-resident database and also serve latency-\\nsensitive end-user requests.\\nWe call a particular instance of a workload a job. A job\\nis composed of one or more tasks. A task is executed on a\\nsingle physical machine; a single machine executes multiple\\ntasks concurrently. A job is a logical entity that corresponds\\nto a service with some functionality (e.g., a filesystem or an\\nauthentication service); tasks do the actual work, such as\\nserving end-user or file-access requests. It is not unusual for\\na job to stay up for months, although during this time we\\nmight perform multiple roll-outs of the binary that the task\\nruns. During such a roll-out, new tasks gradually replace\\ntasks running the older binary.\\nThe workloads we run can be split into two categories:\\nserving and batch. Serving jobs generally aim at strict per-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='tasks running the older binary.\\nThe workloads we run can be split into two categories:\\nserving and batch. Serving jobs generally aim at strict per-\\nformance guarantees on query response time (e.g., a request-\\nlatency service level objective or SLO of ≤ 50 ms at the\\n95%ile). Such tight latency requirements preclude any in-\\nband resource-allocation decisions beyond those of the OS\\nkernel, so serving jobs have the resources they request ex-\\nplicitly set aside for them. In contrast, batch jobs aim to\\nfinish and exit “quickly”, but typically have no strict com-\\npletion deadlines. Serving jobs are the primary driver of our\\ninfrastructure’s capacity, while batch jobs generally fill the\\nremaining or temporarily-unused capacity, as in [4].\\nAn out-of-memory (OOM) event terminates an individ-\\nual task. Some jobs are reasonably tolerant of OOM events;\\nsome are not at all; and some fall in between. Overall, jobs\\ncomposed of more tasks and having less state experience less\\nservice degradation when an individual task terminates, thus\\nare more OOM-tolerant. Some jobs require low, repeatable\\nlatency to serve requests; some do not. Autopilot chooses\\ndefaults based on a job’s declared size, priority and class,\\nbut permits our users to override them. Borg evicts tasks in\\norder to perform security and OS updates, to make space\\nfor higher-priority tasks, and to let us pack work onto the\\nmachines more efficiently. We consciously share the burden\\nof providing service resiliency between the compute cluster\\ninfrastructure and our applications, which are expected to\\nrun additional tasks to work around evictions and hardware\\nfailures. Borg publishes an expected maximum rate of these\\nevictions, and the difference between the observed eviction\\nrate and this gives us the freedom to perform experiments\\nwith a task’s resource settings – an occasional OOM while\\nwe learn what a task needs is OK. (Tools such as VM live'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\nmigration are used to hide these internal optimizations from\\nexternal Cloud VMs.)\\nA typical serving job has many tasks and a load balancer\\nthat drives the traffic to the available ones, so losing a task\\nsimply causes its load to be spread to others. This is normal,\\nnot a catastrophic failure, and enables us to be more aggres-\\nsive about the utilization of our infrastructure, as well as\\nto handle occasional hardware failures gracefully. Similar\\nresilience is built into our batch jobs, using techniques such\\nas those used in MapReduce [6].\\n2.2 Borg scheduler architecture\\nEach cluster is managed by a dedicated instance of Borg, our\\ncustom-built cluster scheduler. Below, we briefly describe\\nthe Borg architecture and features that directly influence\\nAutopilot design; refer to [34] for a complete description.\\nBorg has a replicated Borgmaster that is responsible for\\nmaking scheduling decisions, and an agent process called\\nthe Borglet running on each machine in the cluster. A single\\nmachine simultaneously executes dozens of tasks controlled\\nby the Borglet; in turn, it reports their state and resource\\nusage to the Borgmaster. When a new job is submitted to the\\nBorgmaster, it picks one or more machines where there are\\nsufficient free resources for tasks of the newly submitted job\\n– or creates this situation by evicting lower-priority tasks to\\nmake space. After the Borgmaster decides where to place a\\njob’s tasks, it delegates the process of starting and running\\nthe tasks to the Borglets on the chosen machines.\\n2.3 Resource management through task limits\\nTo achieve acceptable and predictable performance, the tasks\\nrunning on a machine must be isolated from one another.\\nAs with Kubernetes, Borg runs each task in a separate Linux\\ncontainer and the local agent sets the container resource lim-\\nits to achieve performance isolation using cgroups. Unlike\\ntraditional fair-sharing at the OS level, this ensures that a\\ntask’s performance is consistent across different executions\\nof the same binary, different machines (as long as the hard-\\nware is the same) and different neighbors (tasks co-scheduled\\non the same machine) [38].\\nIn our infrastructure, CPU and RAM are the key resources\\nto manage. We use the term limit to refer to the maximum\\npermitted amount of each resource that may normally be\\nconsumed. Since Borg generally treats the jobs’ tasks as\\ninterchangeable replicas, all tasks normally have the same\\nlimits.\\nA job expresses its CPU limit in normalized milli-cores\\n[31], and this limit is enforced by the standard Linux kernel\\ncgroups mechanism. If there is little contention (as measured\\nby the overall CPU utilization), tasks are allowed to use CPU\\nbeyond their limits. However, once there is contention, limits\\nare enforced and some tasks may be throttled to operate\\nwithin their limits.\\nA job expresses its memory limit in bytes. As with the\\nstandard Linux cgroups, the enforcement of a job’s RAM\\nlimit might be hard or soft, and the job’s owner declares the\\nenforcement type when submitting the job. A task using\\na hard RAM limit is killed with an out-of-memory (OOM)\\nerror as soon as the task exceeds its limit, and the failure\\nis reported to the Borgmaster.\\n1 A task using a soft RAM\\nlimit is permitted to claim more memory than its limit, but\\nif the overall RAM utilization on a machine is too high, the\\nBorglet starts to kill tasks that are over their limit (with an\\nOOM error) until the machine is deemed no longer at risk\\n(cf. the standard cgroup enforcement that simply prevents\\nover-the-limit containers reserving more memory).\\nBorg allows a job to modify its resource requirements\\nwhile the job is running. In horizontal scaling a job can dy-\\nnamically add or remove tasks. In vertical scaling, a job can\\nchange its tasks’ RAM and CPU limits. Increasing a job’s\\nRAM and CPU limits is a potentially costly operation, be-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='namically add or remove tasks. In vertical scaling, a job can\\nchange its tasks’ RAM and CPU limits. Increasing a job’s\\nRAM and CPU limits is a potentially costly operation, be-\\ncause some tasks might then no longer fit onto their ma-\\nchines. In such cases, the Borglets on these machines will\\nterminate some lower-priority tasks; these tasks, in turn,\\nwill get rescheduled to other machines and may trigger addi-\\ntional terminations of even lower priority tasks. (A few years\\nago, we drastically reduced the effective number of priority\\nlevels to reduce the amount of this kind of cascading.)\\nAlthough it is common to over-provision a job’s limits,\\nthere is some back-pressure: we charge service users for the\\nresources they reserve, rather than the ones they use, and\\nthe requested resources decrement the user’s quota – a hard\\nlimit on the aggregate amount of resources they can acquire,\\nacross all their jobs, in a cluster. This is similar to the use of\\npricing and quotas for VMs in public clouds. Charging and\\nquotas both help, but in practice have only limited effect: the\\ndownsides of under-provisioning typically far outweigh the\\nbenefits obtained by requesting fewer resources. We have\\nfound this to be a recurring theme: theoretically-obtainable\\nefficiencies are often hard to achieve in practice because the\\neffort or risk required to do so manually is too high. What\\nwe need is an automated way of making the trade-off. This\\nis what Autopilot does.\\n3 Automating limits with Autopilot\\nAutopilot uses vertical scaling to fine-tune CPU and RAM\\nlimits in order to reduce theslack, i.e., the difference between\\nthe resource limit and the actual usage, while ensuring that\\nthe tasks will not run out of resources. It also uses horizontal\\nscaling (changing the number of tasks in a job) to adjust to\\nlarger-scale workload changes.\\n1The standard Linux cgroup behavior is to kill one of the processes executing\\ninside a container, but to simplify failure handling, the Borglet kills all the\\ntask’s processes.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\n3.1 Architecture\\nThe functional architecture of Autopilot is a triple of closed-\\nloop control systems, one for horizontal scaling at the per-job\\nlevel, the other two for vertical scaling of per-task resources\\n(CPU and memory). The algorithms (described in detail in\\nsubsequent sections) act as controllers. Autopilot considers\\njobs separately – there is no cross-job learning.\\nAutopilot’s implementation (Figure 1) takes the form of a\\ncollection of standard jobs on our infrastructure: each cluster\\nhas its own Autopilot. Each of Autopilot’s resource recom-\\nmenders (that size a job according to its historic usage) runs\\nas a separate job, with three task replicas for reliability. A\\nreplicatedAutopilot service, with an elected master, is re-\\nsponsible for selecting the recommender to use for a job and\\npassing (filtered) recommendations to the Borgmaster via an\\nactuator job. If the request is to change the number of tasks,\\nthe Borgmaster spawns or terminates tasks accordingly. If\\nthe request is to change resource limits, the Borgmaster\\nfirsts makes any scheduling decisions needed to accommo-\\ndate them, and then contacts the appropriate Borglet agents\\nto apply the changes. An independent monitoring system\\nkeeps track of how many resources each task uses; Autopilot\\njust subscribes to updates from it.\\nToday, our users explicitly opt-in their jobs to use Au-\\ntopilot, using a simple flag setting; we are in the process of\\nmaking this the default, and instead allowing explicit opt-\\nouts. Users may also configure several aspects of Autopilot’s\\nbehavior, such as: (1) forcing all replicas to have the same\\nlimit, which is useful for a failure tolerant program with just\\none active master; and (2) increasing the limits so that load\\nfrom a replica job in another cluster will be able to fail over\\nto this one instantaneously.\\nWhen a Autopiloted job is submitted to the Borgmaster,\\nit is temporarily queued until Autopilot has had a chance to\\nmake an initial resource recommendation for it. After that,\\nit proceeds through the normal scheduling processes.\\n3.2 Vertical (per task) autoscaling\\nThe Autopilot service chooses the recommender(s) to use for\\na job according to whether the resource being autoscaled is\\nmemory or CPU; how tolerant the job is to out-of-resource\\nevents (latency tolerant or not, OOM-sensitive vs. OOM-\\ntolerant); and optional user inputs, which can include an\\nexplicit recommender choice, or additional parameters to\\ncontrol Autopilot’s behavior, such as upper and lower bounds\\non the limits that can be set.\\n3.2.1 Preprocessing: aggregating the input signal. The\\nrecommenders use a preprocessed resource usage signal.\\nMost of this preprocessing is done by our monitoring system\\nto reduce the storage needs for historical data. The format\\nof aggregated signal is similar to the data provided in [35].\\nThe low-level task monitoring records a raw signal that\\nis a time series of measurements for each task of a job (e.g.,\\nTable 1. Notation used to describe the recommenders\\n𝑟𝑖 [𝜏] a\\nraw, per-task CPU/MEM time series (1s resolution)\\n𝑠𝑖 [𝑡] an aggregated, per-task CPU/MEM time series\\n(histograms, 5 min resolution)\\n𝑠[𝑡] an aggregated, per-job CPU/MEM time series\\n(histograms, 5 min resolution)\\nℎ[𝑡] a per-job load-adjusted histogram\\n𝑏[𝑘] the value of k-th bin (boundary value) of the histogram\\n𝑤[𝜏] the weight to decay the sample aged 𝜏\\n𝑆[𝑡] the moving window recommendation at time 𝑡\\n𝑚 a model (a parametrized arg minalgorithm)\\n𝑑𝑚 the decay rate used by model 𝑚\\n𝑀𝑚 the safety margin used by model 𝑚\\n𝐿 the value of a limit tested by the recommender\\n𝐿𝑚 [𝑡] the limit recommended by model 𝑚\\n𝐿[𝑡] the final recommendation of the ML recommender\\n𝑜(𝐿) the overrun cost of a limit 𝐿\\n𝑢(𝐿) the underrun cost of a limit 𝐿\\n𝑤𝑜 weight of the overrun cost\\n𝑤𝑢 weight of the underrun cost\\n𝑤Δ𝐿 weight of the penalty to change the limit\\n𝑤Δ𝑚 weight of the penalty to change the model'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='𝑢(𝐿) the underrun cost of a limit 𝐿\\n𝑤𝑜 weight of the overrun cost\\n𝑤𝑢 weight of the underrun cost\\n𝑤Δ𝐿 weight of the penalty to change the limit\\n𝑤Δ𝑚 weight of the penalty to change the model\\n𝑑 decay rate for computing the cost of a model\\n𝑐𝑚 [𝑡] the (decayed) historical cost of a model m\\nCPU or RAM usage, or the number of queries received). We\\ndenote the value recorded by the monitoring system at time\\n𝜏 from task 𝑖 as 𝑟𝑖 [𝜏]. This time series typically contains a\\nsample every 1 second.\\nTo reduce the amount of data stored and processed when\\nsetting a job’s limits, our monitoring system preprocesses\\n𝑟𝑖 [𝜏]into an aggregated signal 𝑠[𝑡], which aggregates val-\\nues over, typically, 5 minute windows. A single sample of\\nthe aggregated signal, 𝑠[𝑡], is a histogram summarizing the\\nresource usage of all the job’s tasks over these 5 minutes.\\nMore formally, for each window𝑡, the aggregated per-task\\nsignal 𝑠𝑖 [𝑡]is a vector holding a histogram over the raw\\nsignal 𝑟𝑖 [𝜏]with 𝜏 ∈𝑡. For a CPU signal, the elements of this\\nvector 𝑠𝑖 [𝑡][𝑘]count the number of raw signal samples𝑟𝑖 [𝜏]\\nthat fall into each of about 400 usage buckets: 𝑠𝑖 [𝑡][𝑘]=\\n|{𝑟𝑖 [𝜏]: 𝜏 ∈𝑡∧𝑏[𝑘−1]≤ 𝑟𝑖 [𝜏]< 𝑏[𝑘]}|, where 𝑏[𝑘]is the\\n𝑘-th bucket’s boundary value (these values are fixed by the\\nmonitoring system). For a memory signal we record in the\\nhistogram just the task’s peak (maximum) request during\\nthe 5 minute window (i.e., the per-task histogram 𝑠𝑖 [𝑡][𝑘]\\nhas just one non-zero value). The memory signal uses the\\npeak value rather than the entire distribution because we\\ntypically want to to provision for (close to) the peak memory\\nusage: a task is more sensitive to underprovisioning memory\\n(as it would terminate with an OOM) than CPU (when it\\nwould be just CPU-throttled).\\nWe then aggregate the per-task histograms 𝑠𝑖 [𝑡]into a\\nsingle per-job histogram 𝑠[𝑡]by simply adding 𝑠[𝑡][𝑘]=Í\\n𝑖 𝑠𝑖 [𝑡][𝑘]. We do not explicitly consider individual tasks\\n– e.g., by examining extreme values. Since individual tasks'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\nin most Borg jobs are interchangeable replicas, we use the\\nsame limits for all of them, except in a few special cases.\\n3.2.2 Moving window recommenders. The moving win-\\ndow recommenders compute limits by using statistics over\\nthe aggregated signal 𝑠.\\nWe want the limits to increase swiftly in response to rising\\nusage, but reduce slowly after the load decreases to avoid a\\ntoo-rapid response to temporary downward workload fluc-\\ntuations. To smooth the response to a load spike, we weight\\nthe signal by exponentially-decaying weights 𝑤[𝜏]:\\n𝑤[𝜏]= 2−𝜏/𝑡1/2 , (1)\\nwhere 𝜏 is the sample age and 𝑡1/2 is the half life: the time\\nafter which the weight drops by half. Autopilot is tuned to\\nlong-running jobs: we use a 12 hour half life for the CPU\\nsignal and a 48 hour half life for memory.\\nOne of the following statistics over 𝑠 is used to compute\\nthe recommendation 𝑆[𝑡]at time 𝑡:\\npeak (𝑆max) returns the maximum from recent samples,\\n𝑆max [𝑡]= max𝜏 ∈{𝑡−(𝑁 −1),...,𝑡 }{𝑏[𝑗]: 𝑠[𝜏][𝑗]> 0}, i.e.,\\nthe highest value of a non-empty bucket across the\\nlast N samples, where N is a fixed system parameter.\\nweighted average ( 𝑆𝑎𝑣𝑔) computes a time-weighted av-\\nerage of the average signal value:\\n𝑆𝑎𝑣𝑔 [𝑡]=\\nÍ∞\\n𝜏=0 𝑤[𝜏]𝑠[𝑡−𝜏]\\nÍ𝑁\\n𝜏=0 𝑤[𝜏]\\n, (2)\\nwhere 𝑠[𝜏]is the average usage of histogram 𝑠[𝜏], i.e.,\\n𝑠[𝜏]=\\n\\x10Í\\n𝑗 (𝑏[𝑗]𝑠[𝜏][𝑗])\\n\\x11\\n/\\n\\x10Í\\n𝑗 𝑠[𝜏][𝑗]\\n\\x11\\n.\\n𝑗-%ile of adjusted usage ( 𝑆𝑝 𝑗) first computes a load-ad-\\njusted, decayed histogram ℎ[𝑡]whose 𝑘th element\\nℎ[𝑡][𝑘]multiplies the decayed number of samples in\\nbucket 𝑘 by the amount of load 𝑏[𝑘]:\\nℎ[𝑡][𝑘]= 𝑏[𝑘]·\\n∞Õ\\n𝜏=0\\n𝑤[𝜏]·𝑠[𝑡−𝜏][𝑘]; (3)\\nand then returns a certain percentile 𝑃𝑗 (ℎ[𝑡])of this\\nhistogram. The difference between ℎand the standard\\nhistogram 𝑠is that in𝑠every sample has the same, unit\\nweight, while in ℎthe weight of the sample in bucket\\n𝑘 is equal to the load 𝑏[𝑘].\\nNote that a given percentile of the load-adjusted usage 𝑆𝑝 𝑗\\ncan differ significantly from the same percentile of usage\\nover time. In many cases, we want to ensure that a given\\npercentile of the offered load can be served when the limit\\nis set to accommodate the offered load, rather than simply\\na count of times that instantaneous observed load can be\\nhandled – i.e, we want to weight the calculation by the load,\\nnot the sample count. This difference is illustrated in Figure 2:\\nif the limit was set at 1 (the 90%ile by time) then the 9/19\\nunits of load in the last time instant would be above the limit\\n(lower dashed line). In this case, the load-adjusted histogram\\n90%ile by \\ntime * load\\n90%ile by time\\ntime\\nload\\nFigure 2. The 90%ile of a load signal can be significantly different\\nthan the 90%ile of the integral of the load-time curve. In this exam-\\nple, the 90%ile of the load is 1 unit, using a time-based samples, but\\n10 units if the magnitude of the load is also considered.\\nℎis computed as follows. A single observation counted by\\nthe load-adjusted histogram ℎcan be interpreted as a unit\\nof the signal area processed at a certain load (the current\\nlevel of the signal). ℎis equal to ℎ[1]= 1 ·9 (load of 1 during\\n9 time units) and ℎ[10]= 10 ·10 (load of 10 during 1 time\\nunit). The 90%ile of ℎ, or the limit required so that 90% of\\nsignal area is processed at or below the limit, is therefore 10\\n– which in this case means the whole signal can be processed\\nwithin the limit.\\nAutopilot uses the following statistics based on the signal\\nand the job class. For CPU limits, we use:\\n•batch jobs: 𝑆𝑎𝑣𝑔, the mean, since if a job tolerates CPU\\nthrottling, the most efficient limit for the infrastruc-\\nture is the job’s average load, which allows the job to\\nproceed without accumulating delays.\\n•serving jobs: 𝑆𝑝95, the 95%ile, or 𝑆𝑝90, the 90%ile of\\nload-adjusted usage, depending on the job’s latency\\nsensitivity.\\nFor memory, Autopilot uses different statistics as a func-\\ntion of the job’s OOM tolerance. This is set by default to\\n“low” for most large jobs, and “minimal” (the most stringent)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='sensitivity.\\nFor memory, Autopilot uses different statistics as a func-\\ntion of the job’s OOM tolerance. This is set by default to\\n“low” for most large jobs, and “minimal” (the most stringent)\\nfor small ones, but can be overridden by the user:\\n•𝑆𝑝98 for jobs with low OOM tolerance.\\n•𝑆max for jobs with minimal OOM tolerance.\\n•For jobs with intermediate OOM tolerance, we select\\nvalues to (partially) cover short load spikes; we do this\\nby using the maximum of 𝑆𝑝60, the weighted 60%ile,\\nand 0.5𝑆max, half of the peak usage.\\nFinally, these raw recommendations are post-processed\\nbefore being applied. First, a recommendation is increased\\nby a 10–15% safety margin (less for large limits). Then, we\\ntake the maximum recommendation value seen over the last\\nhour to reduce fluctuations.\\n3.2.3 Recommenders based on machine learning. In\\nprinciple, Autopilot solves a machine learning problem: for\\na job, based on its past usage, find a limit that optimizes a\\nfunction expressing both the job’s and the infrastructure’s\\ngoals. The method described in the previous section (setting'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\na limit based on a simple statistics on a moving window)\\nspecifies an algorithm to solve this problem. In contrast,\\nAutopilot’s ML recommenders start with a cost function – a\\nspecification of the desired solution – and then for each job\\npick appropriate parameters of a model to optimize this cost\\nfunction. Such automation allows Autopilot to optimize for\\neach job the parameters that the previous method fixed for\\nall jobs, such as the decay rate 𝑡1/2, the safety margin or the\\ndownscaling stabilization period.\\nInternally, an ML recommender is composed of a large\\nnumber of models. For each job the recommender periodi-\\ncally chooses the best-performing model (according to the\\ncost function defined below computed over historical usage);\\nthen the chosen model is responsible for setting the limits.\\nEach model is a simple arg min-type algorithm minimizing\\nthe cost – models differ by weights assigned to individual\\nelements of\\narg min. One of the recurring problems of ML\\nmethods is interpretability of their results [8]: in Autopilot,\\nthe limits that the recommender sets must be explainable to\\nthe job owner. Having many simple models helps in inter-\\npreting ML recommender’s actions: a single model roughly\\ncorresponds to inferred characteristics of a job (e.g., models\\nwith long stabilization times correspond to jobs that have\\nrapidly changing utilizations). Then, given the weights im-\\nposed by the chosen model, its decisions are easy to interpret.\\nMore formally, for a signal 𝑠, at time 𝑡 the ML recom-\\nmender chooses from an ensemble of models {𝑚}a single\\nmodel 𝑚[𝑡]that is used to recommend the limits. A model\\nis a parameterized arg minalgorithm that computes a limit\\ngiven historical signal values. A model 𝑚is parameterized\\nby a decay rate 𝑑𝑚 and a safety margin 𝑀𝑚.\\nAt each time instant 𝑡, a model tests all possible limit val-\\nues 𝐿(possible limit values correspond to subsequent bounds\\nof the histogram buckets, 𝐿 ∈ {𝑏[0],...,𝑏 [𝑘]}). For each\\nlimit value 𝐿, the model computes the current costs of under-\\nand overruns based on the most recent usage histogram 𝑠[𝑡]\\nand then exponentially smooths it with the historic value.\\nThe overrun cost 𝑜(𝐿)[𝑡]counts the number of samples in\\nbuckets over the limit 𝐿in the most recent histogram:\\n𝑜(𝐿)[𝑡]= (1−𝑑𝑚)(𝑜(𝐿)[𝑡−1])+𝑑𝑚\\n\\x10Í\\n𝑗:𝑏 [𝑗 ]>𝐿 𝑠[𝑡][𝑗]\\n\\x11\\n. (4)\\nSimilarly, the underrun cost 𝑢(𝐿)[𝑡]counts the number of\\nsamples in buckets below the limit 𝐿,\\n𝑢(𝐿)[𝑡]= (1−𝑑𝑚)(𝑢(𝐿)[𝑡−1])+𝑑𝑚\\n\\x10Í\\n𝑗:𝑏 [𝑗 ]<𝐿 𝑠[𝑡][𝑗]\\n\\x11\\n. (5)\\nThen, a model picks a limit 𝐿′\\n𝑚 [𝑡]that minimizes a weighted\\nsum of overruns, underruns and a penalty Δ(𝐿,𝐿′\\n𝑚 [𝑡 −1])\\nfor a possible change of the limit:\\n𝐿′\\n𝑚 [𝑡]= arg min\\n𝐿\\n\\x10\\n𝑤𝑜𝑜(𝐿)[𝑡]+𝑤𝑢𝑢(𝐿)[𝑡]+𝑤Δ𝐿Δ(𝐿,𝐿′\\n𝑚 [𝑡−1])\\n\\x11\\n,\\n(6)\\nwhere Δ(𝑥,𝑦) = 1 if 𝑥 ≠ 𝑦 and 0 otherwise. (Using the\\nKronecker delta, Δ(𝑥,𝑦)= 1 −𝛿𝑥,𝑦 .) This function captures\\nthe three key costs of making resource allocation decisions\\nin a large-scale system. Overruns express the cost of the\\nlost opportunity – in a serving job when an overrun occurs\\nqueries get delayed, meaning some end-users might be less\\nwilling to continue using the system. Underruns express the\\ncost of the infrastructure: the more resources a job reserves,\\nthe more electricity, machines and people are needed. The\\npenalty termΔ helps avoid changing the limits too frequently,\\nbecause that can result in the task no longer fitting on its\\ncurrent machine causing it (or other tasks) to be evicted.\\nFinally, the limit is increased by the safety margin𝑀𝑚, i.e.,\\n𝐿𝑚 [𝑡]= 𝐿′\\n𝑚 [𝑡]+𝑀𝑚. (7)\\nTo pick a model at runtime (and therefore to optimize the\\ndecay rate 𝑑𝑚 and the safety margin 𝑀𝑚 for a particular\\njob), the ML recommender maintains for each model its\\n(exponentially smoothed) cost 𝑐𝑚 which is a weighted sum\\nof overruns, underruns and penalties for limit changes:\\n𝑐𝑚 [𝑡]= 𝑑\\n\\x10\\n𝑤𝑜𝑜𝑚 (𝐿𝑚 [𝑡],𝑡)+𝑤𝑢𝑢𝑚 (𝐿𝑚 [𝑡],𝑡)+\\n𝑤Δ𝐿Δ(𝐿𝑚 [𝑡],𝐿𝑚 [𝑡−1])\\n\\x11\\n+(1 −𝑑)𝑐𝑚 [𝑡−1].\\n(8)\\nAs historic costs are included in 𝑐𝑚 [𝑡−1], the underrun 𝑢𝑚'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='𝑐𝑚 [𝑡]= 𝑑\\n\\x10\\n𝑤𝑜𝑜𝑚 (𝐿𝑚 [𝑡],𝑡)+𝑤𝑢𝑢𝑚 (𝐿𝑚 [𝑡],𝑡)+\\n𝑤Δ𝐿Δ(𝐿𝑚 [𝑡],𝐿𝑚 [𝑡−1])\\n\\x11\\n+(1 −𝑑)𝑐𝑚 [𝑡−1].\\n(8)\\nAs historic costs are included in 𝑐𝑚 [𝑡−1], the underrun 𝑢𝑚\\nand the overrun 𝑜𝑚 costs for a given model consider only\\nthe most recent costs, i.e., the number of histogram sam-\\nples outside the limit in the last sample, thus 𝑜𝑚 (𝐿𝑚 [𝑡],𝑡)=Í\\n𝑗:𝑏 [𝑗 ]>𝐿 𝑠[𝑡][𝑗], and 𝑢𝑚 (𝐿𝑚 [𝑡],𝑡)= Í\\n𝑗:𝑏 [𝑗 ]<𝐿 𝑠[𝑡][𝑗].\\nFinally, the recommender picks the model that minimizes\\nthis cost, but with additional penalties for switching the limit\\nand the model:\\n𝐿[𝑡]= arg min\\n𝑚\\n\\x10\\n𝑐𝑚 [𝑡]+𝑤Δ𝑚Δ(𝑚[𝑡−1],𝑚)+𝑤Δ𝐿Δ(𝐿[𝑡],𝐿𝑚 [𝑡])\\n\\x11\\n.\\n(9)\\nOverall, the method is similar to the multi-armed bandit\\nproblem with an ‘arm’ of the bandit corresponding to the\\nvalue of the limit. However, the key property of the multi-\\narmed bandit is that once an arm is chosen, we can’t observe\\nthe outcomes of all other arms. In contrast, once the signal is\\nknown for the next time period, Autopilot can compute the\\ncost function for all possible limit values — except in rare\\ncases when the actuated limit turns out to be too small and a\\ntask terminates with an OOM (we show that OOMs are rare\\nin Section 4.3). This full observability makes our problem\\nconsiderably easier.\\nThe ensemble has five hyperparameters: the weights in\\nthe cost functions defined above (𝑑, 𝑤𝑜 , 𝑤𝑢, 𝑤Δ𝐿 and 𝑤Δ𝑚).\\nThese weights roughly correspond with the dollar opportu-\\nnity vs the dollar infrastructure costs. We tune these hyper-\\nparameters in off-line experiments during which we simulate\\nAutopilot behavior on a sample of saved traces taken from\\nrepresentative jobs. The goal of such tuning is to produce a\\nconfiguration that dominates alternative algorithms (such\\nas the moving window recommenders) over a large portion'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\nof the sample, with a similar (or slightly lower) number of\\noverruns and limit adjustments, and significantly higher uti-\\nlization. Such tuning is iterative and semi-automatic: we per-\\nform a parameter sweep (an exhaustive search) over possible\\nvalues of the weights; and then manually analyze outliers\\n(jobs for which the performance is unusually bad). If we con-\\nsider that behavior unacceptable, we manually increase the\\nweight of the corresponding jobs when aggregating results\\nduring the next iteration of the parameter sweep.\\nThese off-line experiments use raw (unadjusted) usage\\ntraces, i.e., they do not try to adjust the signal according\\nto newly set limits (e.g., after an OOM a task should be\\nterminated and then restarted). However, depending on a\\nparticular job, the impact of an OOM or CPU throttling might\\nbe different – for some jobs, an OOM may increase the future\\nload (as a load of the terminated task is taken over by other\\ntasks), while for others, it might result in a decrease (as end-\\nusers fall off when the quality of service degrades). In practice\\nthis is not an issue, because usage-adjusting events are fairly\\nrare, and we continually monitor Autopilot in production,\\nwhere issues like overly-frequent OOMs are easy to spot.\\n3.3 Horizontal autoscaling\\nFor many jobs, vertical autoscaling alone is insufficient: a\\nsingle task cannot get larger than the machine it is running\\non. To address this, Autopilot horizontal scaling dynamically\\nchanges the number 𝑛of tasks (replicas) in a job as a func-\\ntion of the job’s load. The horizontal and vertical scaling\\nmechanisms complement each other: vertical autoscaling\\ndetermines the optimal resource allocation for an individual\\ntask, while horizontal autoscaling adds or removes replicas\\nas the popularity and load on a service changes.\\nHorizontal autoscaling uses one of the following sources\\nto derive the raw recommendation 𝑛𝑟 [𝑡]at time instant 𝑡:\\nCPU utilization: The job owner specifies (1) the aver-\\naging window for the CPU usage signal (the default is\\n5 minutes); (2) a horizon length 𝑇 (the default horizon\\nis 72 hours); (3) statistics 𝑆: max or 𝑃95, the 95%ile;\\nand (4) the target average utilization 𝑟∗. Autopilot\\ncomputes the number of replicas at time 𝑡 from the\\nvalue of 𝑆 for the most recent T utilization samples,\\n𝑟𝑆 [𝑡]= 𝑆𝜏 ∈[𝑡−𝑇,𝑡 ]{Í\\n𝑖 𝑟𝑖 [𝜏]}. Then, the raw recommen-\\ndation for the number of replicas is 𝑛𝑟 [𝑡]= 𝑟𝑆 [𝑡]/𝑟∗.\\nTarget size: the job owner specifies a function 𝑓 for\\ncomputing the number of tasks, i.e., 𝑛𝑟 [𝑡]= 𝑓[𝑡]. The\\nfunction uses data from the job monitoring system.\\nFor example, a job using a queuing system for man-\\naging requests can scale by the 95%ile of the request\\nhandling time; a filesystem server might scale by the\\namount of filespace it manages.\\nHorizontal autoscaling requires more customization than\\nthe vertical autoscaling, which requires no configuration for\\na vast majority of jobs. Even in the standard CPU utilization\\nalgorithm, the job owner has to at least set the target average\\nutilization 𝑟∗(which is similar to how horizontal autoscal-\\ning in public clouds is configured). In our infrastructure,\\nhorizontal autoscaling is used mainly by large jobs. Their\\nowners usually prefer to tune the autoscaler to optimize\\njob-specific performance metrics, such as the 95%ile latency\\n(either directly through specifying target size; or indirectly,\\nby experimentally changing the target average utilization\\nand observing the impact on the metrics).\\nThe raw recommendation 𝑛𝑟 [𝑡]is then post-processed\\nto produce a stabilized recommendation 𝑛𝑠 [𝑡]which aims\\nto reduce abrupt changes in the number of tasks. Autopilot\\noffers a job owner the following choice of smoothing policies\\n(with reasonable defaults for the average job):\\ndeferred downscaling returns the maximum recom-\\nmendation from the 𝑇𝑑 most recent recommendations:\\n𝑛𝑠 [𝑡] = max𝑡−𝑇𝑑,𝑡 {𝑛𝑟 [𝑡]}. Thus, downscaling is de-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='deferred downscaling returns the maximum recom-\\nmendation from the 𝑇𝑑 most recent recommendations:\\n𝑛𝑠 [𝑡] = max𝑡−𝑇𝑑,𝑡 {𝑛𝑟 [𝑡]}. Thus, downscaling is de-\\nferred for the user-specified time 𝑇𝑑 , while upscaling\\nis immediate. Most of our jobs use a long 𝑇𝑑 : roughly\\n40% use 2 days; and 35% use 3 days.\\nslow decay avoids terminating too many tasks simulta-\\nneously. If the current number of tasks 𝑛[𝑡]exceeds\\nthe stabilized recommendation 𝑛𝑠 [𝑡], some tasks will\\nbe terminated every 5 minutes. The number of tasks\\nto terminate at a time is chosen to reduce the number\\nof tasks by half over a given period (98% of jobs use\\nthe default of one hour).\\ndefer small changes is, to some degree, the opposite of\\nthe slow decay: it ignores changes when the difference\\nbetween the recommendation and the current number\\nof tasks is small.\\nlimiting growth allows the job owner to specify a limit\\non the fraction of tasks that are initializing (i.e., haven’t\\nyet responded to health checks), and thus limit the rate\\nat which tasks are added.\\n4 Recommender quality\\nThis section explores how effective Autopilot is at Google,\\nusing samples from our production workloads. We focus on\\nvertical scaling of RAM here because OOMs have such a\\ndirectly measurable impact. We refer to [31] for an overview\\nof the impact of CPU scaling.\\n4.1 Methodology\\nOur results are based on observations made by the monitor-\\ning system that monitors all jobs using our infrastructure.\\nThe large scale of our operation gives us good statistical\\nestimates of the actual gain of Autopilot, but the downside of\\nany purely observational study is that it doesn’t control the\\ntreatment a job receives (Autopilot or manually-set limits),\\nso we needed to compensate for this as far as possible.\\nOne alternative to an observational study would be an\\nA/B experiment, in which we would apply Autopilot to a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\nrandomly chosen half of a sample set of jobs. Although we\\ndid such A/B studies on small groups of jobs, migrating\\nhigh-priority, large, production jobs requires explicit consent\\nfrom the jobs’ owners, so was not practical at a statistically-\\nsignificant scale.\\nAnother alternative would be a simulation study using a\\nrecorded trace, but these have their own biases, and we do not\\nhave a reliable way to predict how a real job would respond\\nto a simulated event such as CPU-throttling (e.g., end users\\nobserving increased latency might disconnect, lowering CPU\\nusage, or reissue their queries, increasing the CPU usage)\\nor an OOM event (e.g., the task might restart and succeed if\\nthe problem was a temporary overload, or simply error out\\nagain if it was caused by a memory leak).\\nTo mitigate the problems of observability studies, we use\\nresults sampled from several different job populations.\\nThe first population is a (biased) sample of 20 000 jobs\\nchosen randomly across our fleet. We sample 5 000 jobs each\\nfrom the following four categories: jobs with manually-set\\nlimits that use hard RAM limits; jobs with manually-set limits\\nthat use soft RAM limits; jobs that use the Autopilot moving\\nwindow recommender; and jobs that use the Autopilot ML\\nrecommender. This population gives us fleet-wide measures\\nof the effects of Autopilot, provided we control for a few\\npotential issues:\\n•Most jobs with manually-set RAM limits use hard lim-\\nits, whereas Autopilot switches all its jobs to soft RAM\\nlimits. This switch might itself reduce the number of\\nOOMs2. We mitigate this problem by sampling equal\\nnumbers of jobs with hard and soft RAM limits.\\n•A job might be forced to use manual limits because\\nAutopilot had trouble setting its limits correctly. We ad-\\ndress this problem by using a second population which\\nis comprised of a sample of 500 jobs that started to use\\nAutopilot in a particular calendar month. We report\\ntheir performance during the two calendar months\\njust before and just after the change, to mitigate the\\nrisk that binaries or load characteristics might have\\nchanged. Even this population could be biased because\\nwe can only sample jobs that were successfully mi-\\ngrated, but our success rate is high, so we do not be-\\nlieve this is a significant concern.\\n4.1.1 Metrics. The performance metrics we report on are\\nbased on samples taken over 5-minute aggregation windows\\n(the default for our monitoring system), across calendar days\\n(to align with how we charge for resource allocations), and\\ntypically use 95th percentiles, to achieve an appropriate bal-\\nance between utilization and OOM rates. The metrics are as\\nfollows:\\n2Memory leaks can be found more quickly with hard limits; an internal\\nuser survey told us that most preferred them for non-Autopiloted jobs.\\nfootprint for a job during a calendar day is the sum of\\nthe average limits of the tasks (each task weighted by\\nits runtime during that day). The footprint directly\\ncorresponds to the infrastructure costs of a job: once\\na task requests resources, other high-priority tasks\\ncannot reclaim them. Footprint is expressed in bytes;\\nhowever, we normalize it by dividing the raw value\\nin bytes by the amount of memory a single (largish)\\nmachine has. So, if a job has a footprint of 10 machines,\\nit means that it is allocated the amount of RAM equal\\nto that of 10 machines (it does not mean it is allocated\\non 10 machines exclusively dedicated to this job).\\nrelative slack for a job during a calendar day is (limit\\nminus usage) divided by limit – i.e., the fraction of re-\\nquested resources that are not used. Here, usage is the\\n95%ile of all 5-minute average usage values reported\\nfor all of a job’s tasks during a calendar day, and limit\\nis the average limit over that 24 hour period.\\nabsolute slack for a job during a calendar day (mea-\\nsured in bytes) directly measures waste: it is the sum\\nof limit-seconds minus usage-seconds over all tasks of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='absolute slack for a job during a calendar day (mea-\\nsured in bytes) directly measures waste: it is the sum\\nof limit-seconds minus usage-seconds over all tasks of\\na job, divided by 24 ×3600 (one day). This aggregation\\nputs more emphasis on larger, more costly, jobs. Here,\\nlimit-seconds is the integral of the requested memory\\nlimit across the running time of the tasks, using the 5\\nminute averages. We normalize absolute slack as we\\nnormalize footprint, so if the total absolute slack for\\nan algorithm is 50, we are wasting the amount of RAM\\nequal to that of 50 machines. Achieving a small value\\nof absolute slack is an ambitious target: it requires that\\nall tasks have their limits almost exactly equal to their\\nusage at all times.\\nrelative OOM rate is the number of out-of-memory\\n(OOM) events experienced by a job during a day, di-\\nvided by the average number of running tasks the\\njob has during that day. It is directly related to how\\nmany additional tasks our users need to add to a job\\nto tolerate the additional unreliability imposed on it\\nby Autopilot. Since OOMs are rare, we also track the\\nnumber of job-days that experience no OOMs at all.\\nThe metrics are reported for job-days (i.e., each job will\\nreport 30 or 31 such values in a calendar month), and cal-\\nculate statistics (e.g., the median relative slack) over all the\\nreporting days for all the jobs.\\nAutopilot may hit a scaling limit when it tries to increase\\nthe limit for a job (e.g., the task becomes larger than the\\navailable quota, or a user-specified boundary, or even a single\\nmachine’s size). We did not filter out such OOMs as it is not\\nclear what this job’s future behavior would be, and the impact\\nof such events should be independent of the algorithm used.\\n4.1.2 Job sampling and filtering. We show performance\\nof a sample of jobs executing on our infrastructure in a\\nsingle calendar month (or 4 months, in case of the analysis of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\nmigrated jobs). While our infrastructure runs many types of\\njobs, its size is driven by high-priority, serving jobs, as such\\njobs are guaranteed to get the resources they declare as their\\nlimits. Thus, tighter limits translate directly to more capacity\\nand a reduced rate of future infrastructure expansion. Our\\nanalysis therefore focuses on these jobs.\\nUnless otherwise noted, we consider only long-running\\njobs (services that are serving for at least the whole calendar\\nmonth) as these jobs have the most significant impact on our\\ninfrastructure [26]. We also filter out a few job categories\\nwith special-purpose, unusual SLOs, and jobs using custom\\nrecommenders, to focus the discussion on the quality of the\\ndefault algorithms.\\n4.2 Reduction of slack\\nAutopiloted jobs have significantly lower slack than non-\\nAutopiloted jobs. Figure 3a shows the cumulative distribution\\nfunction (CDF) of slack per job-days. The average relative\\nslack of a non-Autopiloted job ranges from 60% (hard limits)\\nto 46% (soft limits); while the average relative slack of a\\nAutopiloted job ranges from 31% (moving window) to 23%\\n(ML).\\nNon-Autopiloted jobs waste significant capacity. Figure 3b\\nshows the cumulative distribution function of absolute slack\\nby jobs within our sample. The total absolute slack summed\\nover our sample of 10 000 non-Autopiloted jobs (and aver-\\naged over the month) is equal to more than 12 000 machines;\\nwhile the absolute slack of the sample of Autopiloted jobs is\\nless than 500 machines. The difference corresponds to tens\\nof millions of USD of machine costs.\\nThese comparisons might be biased, however, as when\\nconstructing those samples we controlled thenumber of jobs\\nfrom each category, not the total amount of resources: if all\\nAutopiloted jobs had small usage, and all non-Autopiloted\\njobs had large usage, we might end up with a similar savings\\nof absolute slack regardless of the quality of limits in each\\ngroup (however, the relative slack comparisons are still valid).\\nTo address this, we show the CDF of the footprint of jobs\\nin Figure 3c. This plot confirms that Autopiloted jobs do\\nhave smaller footprints compared to non-Autopiloted jobs.\\nHowever, as we will see when analyzing jobs that migrated\\nto Autopilot, this smaller footprint is, at least partially, a\\nconsequence of Autopilot reducing jobs’ limits. Moreover,\\nsmall jobs use Autopilot by default (Section 5).\\nFinally, we analyze the reduction of slack in jobs that\\nrecently started to use Autopilot (Figure 4). Almost all jobs\\nused hard memory limits before the migration; and almost\\nall use the ML recommender after the migration. Our plots\\nshow results over 4 months of jobs’ lifetime. All jobs start\\nto use Autopilot in the same calendar month, denoted as\\nmonth 0 (m0). We show the performance of these jobs over\\nthe two previous months, denoted as m-1 and m-2 when jobs\\nused manual limits; and also performance over two months\\nfollowing the migration, denoted as m+1 and m+2 when jobs\\nused limits set by Autopilot.\\nFigure 4a shows the CDF for relative slack per job-days.\\nIn the month before the migration, the average relative slack\\nwas 75%, with a median of 84%. In the month following\\nthe migration, the average relative slack decreased to 20%\\nand the median decreased to 17%. The distribution of slack\\nvalues remains consistent in the two months following the\\nmigration, suggesting that the gains are persistent.\\nThe absolute slack (Figure 4b) show significant savings:\\nbefore the migration, these jobs wasted an amount of RAM\\nequal to the capacity of 1870 machines; after the migration,\\nthe jobs waste only 162 machines: by having migrated these\\njobs, we saved the capacity of 1708 machines.\\nThe CDF of migrated jobs’ footprint (Figure 4c) shows that\\nthe footprint of jobs increases in time suggesting organic\\ngrowth in traffic. The total footprint of jobs two months be-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='The CDF of migrated jobs’ footprint (Figure 4c) shows that\\nthe footprint of jobs increases in time suggesting organic\\ngrowth in traffic. The total footprint of jobs two months be-\\nfore the migration was smaller than in the month before the\\nmigration; similarly, the total footprint in the month after the\\nmigration was smaller than the footprint two months after\\nthe migration. Migration in m0 reversed this trend: while\\nfootprint organically grows month-by-month, the footprint\\nin m+1 was notably smaller than the footprint in m-1. After\\nmigration, the rate by which the footprint grows was also\\nreduced, as the CDF of m+2 is closer to m+1 than the CDF\\nof m-2 is to m-1.\\nThe distribution of footprint of 500 migrated jobs (Fig-\\nure 4c) differs from the footprint of the 20 000 jobs sampled\\nacross our fleet (Figure 3c): the migrated jobs have a larger\\nfootprint than the fleet as a whole. This is because many\\nsmall jobs were automatically migrated earlier than m0, the\\nmonth we picked as the reference month for this sample (see\\nSection 5 for details).\\n4.3 Reliability\\nThe previous section demonstrated that Autopilot results in\\nsignificant reduction of wasted capacity. However, a trivial\\nalgorithm, setting the limit to 0, would result in even better\\nresults by these metrics – at the expense of frequent OOMs!\\nIn this section we show that Autopiloted jobs have higher\\nreliability than the non-Autopiloted ones.\\nFigure 5 shows the cumulative distribution function (CDF)\\nof relative OOMs by job-days. OOMs are rare: over 99.5%\\nof Autopilot job-days see no OOMs. While the ML recom-\\nmender results in slightly more OOM-free job-days than\\nthe moving window recommender, it also leads to slightly\\nmore relative OOMs (0.013 versus 0.002 per task-day). Both\\nalgorithms clearly dominate manual limit setting. With hard\\nRAM limits, around 98.7% job-days are OOM-free; and the\\nrelative OOM rate is 0.069/task-day. Soft RAM limit jobs\\nhad a better relative OOM rate of 0.019, but slightly fewer\\nOOM-free job-days (97.5%).\\nThe number of OOMs naturally depends on the relative\\nslack – higher slack means that more memory is available,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\n0.0 0.2 0.4 0.6 0.8 1.0\\nmemory slack, p95\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot ML\\nAutopilot moving window\\nmanual, hard limit\\nmanual, soft limit\\n(a) CDF of relative slack (fraction of claimed,\\nbut unused memory).\\n10−2 10−1 100 101 102\\nabsolute slack [machines]\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot ML\\nAutopilot moving window\\nmanual, hard limit\\nmanual, soft limit\\n(b) CDF of absolute slack (amount of claimed,\\nbut unused memory).\\n10−2 10−1 100 101 102\\njob footprint [machines] \\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot ML\\nAutopilot moving window\\nmanual, hard limit\\nmanual, soft limit\\n(c) CDF of the footprint (total limit).\\nFigure 3. Resource usage. CDFs over job-days of a random sample of 5 000 jobs in each category, drawn from across the entire fleet.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nmemory slack, p95\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot m+1\\nAutopilot m+2\\nmanual m-2\\nmanual m-1\\n(a) CDF of relative slack (fraction of claimed,\\nbut unused memory)\\n10−2 10−1 100 101 102\\nabsolute slack [machines]\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot m+1\\nAutopilot m+2\\nmanual m-2\\nmanual m-1\\n(b) CDF of absolute slack (amount of claimed,\\nbut unused memory)\\n10−2 10−1 100 101 102\\njob footprint [machines]\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAutopilot m+1\\nAutopilot m+2\\nmanual m-2\\nmanual m-1\\n(c) CDF of the footprint (total limit)\\nFigure 4. Resource usage. CDF over job-days. 500 jobs that migrated to Autopilot.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nrelative OOMs\\n0.975\\n0.980\\n0.985\\n0.990\\n0.995\\n1.000\\nAutopilot ML\\nAutopilot moving window\\nmanual, hard limit\\nmanual, soft limit\\nFigure 5. Cumulative distribution function of relative OOMs (num-\\nber of OOMs per day normalized by the number of tasks) by job-\\ndays. Note non-zero y-axis offset – the vast majority of jobs-days\\nhave no OOMs, e.g., in the Autopilot cases, over 99.5% of job-days\\nare OOM-free.\\nso a task should OOM more rarely. The line slopes in Fig-\\nure 6 represent how strongly the OOM rates relate to slack,\\nwhile the intercepts reflect the overall number of OOMs. The\\nregression line for non-Autopiloted jobs with soft limits falls\\nbelow non-Autopiloted jobs with hard limits; there is a simi-\\nlar strict dominance between Autopiloted jobs that use the\\nFigure 6. A scatterplot showing OOMs vs slack. A point corre-\\nsponds to a single job-day; the point’s color shows how limit is set\\nfor that job on that day. Lines (with a 95%ile confidence interval\\nband) show linear regressions.\\nmoving window algorithm and the non-Autopiloted jobs us-\\ning soft limits. However, the regression for the ML algorithm\\nintersects the lines for jobs using a manually specified soft\\nlimit and those using the moving window Autopilot scheme,\\nsuggesting more OOMs for jobs with low slack – but also\\nfewer OOMs for jobs with higher slack.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\nTable 2. Number of job-days seriously impacted by OOMs, by algo-\\nrithm. Each row corresponds to a different threshold for classifying\\na job-day as being seriously impacted by OOMs: e.g., in the first\\nrow, a job-day is seriously impacted if 5, or 1/7th of the job’s tasks\\n(whichever is higher) are terminated with an OOM. Both “hard”\\nand “soft” have manual limit settings.\\nthresholds affected job-days\\n# OOMs fraction tasks hard soft window ML\\n5 1/7 807 994 131 98\\n4 1/5 813 945 120 108\\n4 1/7 862 1059 142 109\\n4 1/10 916 1235 152 113\\n3 1/7 931 1116 149 118\\nBecause the ML recommender results in higher average\\nrelative OOMs rates than the sliding window recommender,\\nit may be that the ML recommender reduces jobs’ limits too\\naggressively. However, the ML recommender is designed\\nto result in an occasional OOM for jobs that won’t be im-\\npacted too much. As we explained in Section 2, our jobs are\\ndesigned to absorb occasional failures – as long as there are\\nsufficiently many surviving tasks able to absorb the traffic\\nthat the terminated task once handled. This is working as\\nintended, and the benefit is bigger resource savings. Yet one\\nmight reasonably still be concerned that the recommender\\nis being too aggressive.\\nTo explore this concern, we categorized a job-day as be-\\ning seriously impacted by OOMs when it experiences more\\nOOMs during the day than the larger of a threshold number\\n(e.g., 4) or fraction (e.g., 1/7) of its tasks. Table 2 shows the\\nnumber of job-days that were seriously impacted by OOMs\\nacross some threshold settings ranging from more liberal\\n(top) to more conservatitive (bottom). Although the absolute\\nnumbers slightly vary, the relative ordering of the methods\\nstays the same.\\nAmong non-Autopiloted jobs, we were surprised to find\\nthat the jobs with hard RAM limits, while having more rela-\\ntive OOMs (as discussed above), were less seriously affected\\nby OOMs. We hypothesize that users may be manually spec-\\nifying soft limits for jobs with an erratic memory usage\\npattern that are particularly difficult to provision for. Among\\nAutopiloted jobs, as expected, while jobs using the moving\\nwindow algorithm have less relative OOMs, they are some-\\nwhat more likely to be seriously affected by OOMs than jobs\\nusing the ML algorithm.\\nWe also studied in more detail how concentrated OOMs\\nare across jobs. If most OOMs occur in just a few jobs, this\\nmight point to a systematic problem with the autoscaling\\nalgorithm — we would rather have many jobs experiencing\\ninfrequent OOMs. We analyze jobs that had at least one OOM\\nduring the month. For each such job, we count the days with\\nat least one OOM (we count OOM-days, rather than simply\\nOOMs or relative OOMs, to focus on the repeatability, rather\\n0 1 2 3 4 5 6 7 8 9\\nNumber of limit changes per day\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nmoving window\\nML\\nFigure 7. CDF of the number of limit changes per job-day. Note\\nnon-zero y-axis offset.\\nthan magnitude, which we measured above). For Autopilot\\nML, 46% of such jobs OOM exactly once; and 80% of jobs\\nOOM during 4 days or less. In contrast, in Autopilot moving\\nwindow recommender, only 28% of jobs OOM exactly once;\\nand 80% of jobs OOM during 21 days or less.\\nIn our second sample population (jobs that migrated to\\nAutopilot), the number of OOMs is too small for meaningful\\nestimates of relative OOMs and serious OOMs. In the month\\nbefore the migration, there were in total 348 job-days in\\nwhich there is at least one OOM; after the migration, this\\nnumber was reduced to just 48. Migration was successful for\\nthese jobs.\\n4.4 Number of limit changes\\nManually-controlled jobs rarely have their limits changed:\\non our sample of 10 000 manually-limited jobs, we observed\\n334 changes during one month, or about 0.001 changes per\\njob-day. Figure 7 shows how often Autopilot changes limits\\non our 10 000 job sample: a few hundred times more often'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='334 changes during one month, or about 0.001 changes per\\njob-day. Figure 7 shows how often Autopilot changes limits\\non our 10 000 job sample: a few hundred times more often\\nper job than users do. However, it is still quite stable: in\\nroughly 70% of job-days there are no changes; and the 99%ile\\njob-day has only 6 (moving window) to 7 (ML) limit changes\\nduring a day. Given that it typically only takes a few tens\\nof seconds to find a new place for a task even if it is evicted,\\nthis seems a reasonable price to pay for significant savings.\\nOne might argue that the reduction of OOMs (and serious\\nOOMs) reported in the previous section comes just from\\nchanging the limits more often than a human operator. In\\nFigure 7, Autopilot ML and moving window have a similar\\nnumber of limit changes; yet, as Table 2 shows, Autopilot\\nML uses this disruption budget more efficiently.\\n4.5 Behavior in time\\nIn previous sections, we focused on long-running jobs: our\\njobs were serving continuously for at least a month. In this\\nsection, we analyze Autopilot performance as a function of\\nthe jobs’ age. Figure 8a shows CDFs of the relative slack for\\n1 000 jobs of each different age range.\\nJobs that have been running for less than a day have a\\nsignificantly higher slack than jobs that have been running\\nlonger: this is a direct result of Autopilot being tuned for\\nlong-running jobs – the more history is available, the lower'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\n0.0 0.2 0.4 0.6 0.8 1.0\\nmemory slack, p95\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nStarted this day\\nStarted previous day\\nStarted 2 days ago\\nStarted 7 days ago\\nStarted 14 days ago\\n(a) CDF of memory slack for different jobs ages.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nrelative OOMs\\n0.970\\n0.975\\n0.980\\n0.985\\n0.990\\n0.995\\n1.000\\nStarted this day\\nStarted this day short\\nStarted this day, long\\nStarted previous day\\nStarted 2 days ago\\nStarted 7 days ago\\nStarted 14 days ago\\n(b) CDF of relative OOMs (number of OOMs per day normalized by\\nnumber of tasks) for different job ages. Note non-zero y-axis offset.\\nFigure 8. Performance over jobs with short history. Each different\\nage range has 1 000 jobs sampled from our whole fleet.\\nthe slack. But even after 14 days the slack is still higher than\\nfor the steady state analyzed in the previous section.\\nThe analysis of relative OOM rates (Figure 8b) shows that\\nAutopilot is cautious with short jobs. For jobs with less than\\n24h duration, there are almost no OOMs: however, if we filter\\nout short jobs (ones with total task duration of less than 1.5\\nhours), there are more OOMs than in the steady state. Once\\nwe consider jobs that started 7 or more days ago, the relative\\nOOM rates are comparable to the steady state behavior.\\n5 Winning the users’ trust: key features\\nfor increasing adoption\\nOur infrastructure serves thousands of internal users who\\nhave varied roles, experience and expectations. Smaller or\\nnewer services are typically run in production by software\\nengineers who originally created them, while larger, more\\nestablished services have dedicated dev/ops teams. To in-\\ncrease Autopilot’s adoption we had to not only make sure\\nthe quality of our algorithms was acceptable, but also identify\\nand answer needs our engineers have from the infrastruc-\\nture. This section discusses these qualitative aspects. Our\\nexperience reinforces many of the lessons described in [5].\\n5.1 Evaluation process\\nAlong with Autopilot, we developed a process to evaluate\\npotential recommenders. A recommender is first evaluated\\nin off-line simulations, using traces of resource usage of a\\nrepresentative sample of jobs. While such evaluation is far\\nfrom complete (we detailed problems in Section 4.1), it is\\ngood enough to determine whether it is probably worth\\ninvesting more effort in a recommender. If so, we proceed to\\nusing dry runs, in which the recommender runs as part of the\\nproduction Autopilot service along side other recommenders\\n– its recommendations are logged, but not acted upon. In both\\nphases, we analyze the usual statistical aggregations such as\\nmeans and high percentiles, but also pay particular attention\\nto outliers – jobs on which the recommender performed\\nparticularly badly. These outliers have helped catch both\\nbugs in implementation and unintended consequences of\\nour algorithms.\\nAfterwards, we perform A/B tests in which the new\\nrecommender drives limits in production for a small fraction\\nof users within a chosen cluster. Even a complete algorithm\\nfailure in this phase is unlikely to be catastrophic: if a job\\nfails in one cluster, the service’s load balancer will switch its\\ntraffic to other clusters, which should have enough capacity\\nto handle the surge.\\nFinally, when the new recommender compares favorably\\nin A/B tests, we gradually deploy it as a new standard for the\\nentire fleet. To reduce the risk of possible failures, roll-outs\\nare performed cluster by cluster, with multi-hour gaps be-\\ntween them, and can be rolled back if anomalies are detected.\\n5.2 Autopilot limits easily accessible to job owners\\nOur standard dashboard (Figure 9) for resource monitoring\\ndisplays the distribution of job CPU and memory usage as\\nwell as the limits Autopilot computed – even for jobs that\\nare not Autopiloted (for these jobs Autopilot runs in simu-\\nlation mode). The dashboard helps the user to understand\\nAutopilot’s actions, and build trust in what Autopilot would'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='are not Autopiloted (for these jobs Autopilot runs in simu-\\nlation mode). The dashboard helps the user to understand\\nAutopilot’s actions, and build trust in what Autopilot would\\ndo if it was enabled on non-Autopiloted jobs: the user can\\nsee how Autopilot would respond to daily and weekly cycles,\\nnew versions of binaries or suddenly changing loads.\\n5.3 Automatic migration\\nOnce we had sufficient trust in Autopilot’s actions from\\nlarge-scale off-line simulation studies and smaller-scale A/B\\nexperiments, we enabled it as a default for all existing small\\njobs (with an aggregate limit of up to roughly 10 machines)\\nand all new jobs. Users were notified well in advance and\\nthey were able to opt-out. This automatic migration trivially\\nincreased adoption with practically no user backlash.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\n(a) A trace of an Autopiloted job. Just after 8:00, some tasks started to exhibit an increased level of memory usage; Autopilot\\npromptly increased the job’s limit.\\n(b) A trace of a job using manually specified limits and with Autopilot running in simulation mode. A new roll-out had a\\nmemory leak that caused the memory usage to steadily increase. At about 17:30, the job’s monitoring system started to page\\non-calls as the job started to OOM. It was not until 19:30 that the on-call dev/ops engineer managed to increase the job’s\\nmemory limit (limit oscillations between 18:20 and 19:00 are an artifact of misalignment of time series data from the monitoring\\nsystem). If the job had been Autopiloted, it would probably not have OOMed as the automatically increased limits would have\\ngiven dev/ops more time to discover the memory leak and roll back to the previous version of the binary.\\nFigure 9. Screenshots of our monitoring dashboards for two production jobs. The dashboard includes a color-coded heatmap which indicates\\nthe number of a job’s tasks with a certain usage at a particular time moment.\\n5.4 Overriding recommenders with custom\\nrecommenders\\nAutopilot’s algorithms rely on historical CPU/memory usage\\nto set future limits or task counts. However, for some jobs\\nother signals are better predictors of the limits: for instance,\\nthe load of our filesystem servers depends almost linearly\\non the total size of the files controlled by the server. Addi-\\ntionally, some long-running services had already developed\\ntheir own horizontal autoscalers before Autopilot, some of\\nwhich contained sophisticated, finely-tuned logic that had\\nbeen refined over many years (although they often had only\\na subset of Autopilot’s features, and they did not always keep\\nup with changes in Borg). Autopilot’s custom recommenders\\npermit users to preserve the critical parts of such algorithms\\n– computation of the number of tasks or individual task re-\\nsource limits – while delegating support functions such as\\nactuation to the Autopilot ecosystem.\\nCustom recommenders proved popular: 3 months after\\nthey were made available, custom recommenders were man-\\naging 13% of our entire fleet’s resources.\\n6 Reducing engineering toil\\nGoogle follows the dev/ops principle of reducing toil: tedious,\\nrepetitive work should be performed by machines rather than\\nengineers and so we invest in automation to do so. Autopilot\\nis one example.\\nA job’s limits need to be increased as the job’s work-\\nload increases. Popular services, even excluding the initial\\nrapid growth phase, probably should be resized bi-weekly\\nor monthly. And each roll-out of a new binary version may\\nrequire limit adjustments. Suppose these were done manu-\\nally. We assume that a manual resize requires on average\\n30 minutes of work: to change the configuration file, sub-\\nmit the change to the version control system, review the\\nproposed change, initialize the roll-out and monitor its re-\\nsults. For our sample of 10 000 jobs with manual limits, even\\nthe 334 manual limit adjustments represent a total toil of\\nroughly a person-month – and this is significantly less than\\nthe expected number of updates.\\nAutopilot’s horizontal scaling – adding tasks to running\\njobs – automatically handles organic load growth. Autopi-\\nlot’s vertical scaling can handle both per-task load changes'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\nand the effects of rolling out of a new binary. Both represent\\nsignificant toil reduction.\\nWe asked several owners of large jobs who migrated to Au-\\ntopilot to estimate the reduction in toil they had experienced.\\nOne large service (composed of multiple jobs) reported that\\nbefore migrating to Autopilot they performed roughly 8 man-\\nual resizes monthly. Another service estimated that Autopilot\\nsaves them 2 hours of human work per month previously\\nneeded for manual resizes. Another service, for which load\\nvaried significantly between clusters and in time, needed\\nabout 12 manual resizes per month.\\nAnother benefit is reducing the interrupts (pages) that\\nmust be handled by on-call dev/ops engineers. With in-\\ncreased reliability, tasks fail less often, and the reporting\\nsystem issues fewer alarms. This reduction is especially pro-\\nnounced for jobs with loads that vary significantly across\\nclusters: the toil of setting different manual limits is a fre-\\nquent source of problems, and even complicates monitoring.\\nOne service reported that after migration, Autopilotincreased\\nmemory limits in some clusters, which resulted in reducing\\nthe number of OOMs from roughly 2 000 a day to a negligi-\\nble number. Another service reported no OOMs for almost a\\nyear following the migration; the number of on-call pages\\nwas reduced from 3 per week to less than 1 (the remaining\\npages were for unrelated problems).\\nAutopilot’s tighter resource limits may expose bugs in\\njobs that went unnoticed with larger limits. Rare memory\\nleaks or out-of-bounds accesses are notoriously difficult to\\nfind. While Autopilot works well in most cases, it may still\\nrequire customized configuration for a few jobs. Thus, when\\na job is migrated to Autopilot and then starts to OOM fre-\\nquently, it can be difficult to distinguish between Autopilot\\nmis-configurations and a true bug. One group blamed Au-\\ntopilot’s memory limit setting algorithm for such a problem,\\nand only discovered the root cause a few weeks later: a\\nrarely-triggered out-of-bounds memory write.\\nAnd finally, Autopilot is used heavily by batch jobs (88% of\\nsuch jobs by CPU enable it). We surmise that this is because\\nAutopilot eliminates the need for a user to even specify limits\\nfor such jobs.\\n7 Related work\\nWhile Autopilot actuation, UX and some customisations are\\nspecific to Borg, the problem Autopilot solves is almost uni-\\nversal in cloud resource management.\\nThe desired number of replicas and their resource require-\\nments are expected to be provided by users in many cloud\\nresource management systems, as the scheduler uses them\\nto pack tasks to machines [14]. Borg [34], Omega [29] and\\nKubernetes [3] all require their users to set such limits when\\nsubmitting jobs (Borg, Omega) or pods (Kubernetes). YARN\\n[32] requires applications (jobs) to state the number of con-\\ntainers (tasks) and the CPU and RAM resource requirements\\nof each container. In a somewhat different context, sched-\\nulers for HPC systems, such as Slurm [37], require each batch\\njob to specify the number of machines.\\nOther studies confirm low utilization in private clouds that\\nwe observed in our infrastructure when jobs have their limits\\nmanually set. [30] analyzes 5-day usage of a 10 000-machine\\nYARN cluster at Alibaba, and reports that 80% of time, RAM\\nutilization is less than 55%. [23] analyzes a short (12-hour)\\nAlibaba trace, showing that for almost all instances (tasks),\\nthe peak memory utilization is 80% or less. [26], analyzing\\nthe 30-days Google cluster trace [27], shows that although\\nthe mean requested memory is almost equal to the total\\navailable memory, the actual usage (averaged over one-hour\\nwindows) is below 50%.\\nAn alternative to setting more precise limits is to oversub-\\nscribe resources, i.e., to deliberately assign to a machine tasks\\nsuch that the sum of their requirements is higher than the\\namount of physical resources available locally. [30] shows a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='scribe resources, i.e., to deliberately assign to a machine tasks\\nsuch that the sum of their requirements is higher than the\\namount of physical resources available locally. [30] shows a\\nsystem oversubscribing resources in a YARN cluster. While\\noversubscription can be used in batch workloads that can\\ntolerate occasional slowdowns, it may lead to significant in-\\ncreases in tail latency for serving workloads – which requires\\na careful, probabilistic treatment [2, 21].\\nHorizontal and vertical autoscaling requires the job to be\\nelastic. In general, many classes of applications are notori-\\nously difficult to scale. For example, JVMs, in their default\\nconfiguration, are reluctant to release heap memory. Fortu-\\nnately for Autopilot, the vast majority of jobs at Google were\\nbuilt with scaling in mind.\\nAutoscaling is a well-developed research area; recent sur-\\nveys include [11, 16, 22]. The majority of research addresses\\nhorizontal autoscaling. [17] experimentally analyzes the per-\\nformance of a few horizontal autoscaling algorithms for\\nworkflows. [10] builds probabilistic performance models of\\nhorizontal autoscalers in AWS and Azure. [ 24] measures\\nthe performance of horizontal autoscalers in AWS, Azure\\nand GCE. While Autopilot also has a reactive horizontal au-\\ntoscaler, this paper largely concentrates on vertical scaling\\n(also called rightsizing, or VM adaptation in [16]). Kubernetes\\nvertical pod autoscaler (VPA, [15]) sets containers’ limits us-\\ning statistics over a moving window (e.g., for RAM, the 99th\\npercentile over 24h). Kubernetes’ approach was directly in-\\nspired by our moving window recommenders (Section 3.2.2).\\n[\\n25] proposes an estimator which uses the sum of the median\\nand the standard deviation over samples from a window.\\nWe described two recommenders: one based on statistics\\ncomputed from a moving window, with window parame-\\nters, such as length, set by the job owner (Section 3.2.2); and\\nanother one that automatically picks the moving window\\nparameters based on a cost function (Section 3.2.3). An al-\\nternative to these simple statistics would be to use more\\nadvanced time series forecasting methods, such as autore-\\ngressive moving average (ARMA) (as in [28] that also uses a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='Autopilot: workload autoscaling at Google EuroSys ’20, April 27–30, 2020, Heraklion, Greece\\njob performance model), neural networks (as in [18]), recur-\\nrent neural networks as in [9, 20] or a custom forecast as in\\n[13] which demonstrates that Markov-chain based prediction\\nperforms better than methods based on autoregression or\\nautocorrelation. Our initial experiments with such methods\\ndemonstrated that, for the vast majority of Borg cases, the\\nadditional complexity of ARMA is not needed: jobs tend to\\nuse long windows (e.g., the default half life time for memory\\nin moving window recommenders is 48 hours, Section 3.2.2);\\nand the between-day trends are small enough that a simple\\nmoving window reacts quickly enough. For a recommender\\nthat is configurable by the user we believe that it is more\\nimportant that parameters have simple semantics and that\\nthe recommender can be tuned predictably.\\nAutopilot refrains from building a job performance model:\\nit does not try to optimize batch jobs’ completion time or\\nserving jobs’ end-user response latency. We found that con-\\ntrolling the limits enables job owners to reason about job\\nperformance (and performance problems) and to separate\\nconcerns between the job and the infrastructure. This sepa-\\nration of concerns partly relies on the cluster and node-level\\nschedulers. For example, Autopilot does not need to con-\\nsider performance problems from so-called noisy neighbors\\nas Borg handles them through a special mechanism [ 38].\\nTo cover the few remaining special cases, Autopilot pro-\\nvides horizontal scaling hooks to allow teams to use custom\\nmetrics or even custom recommenders. In contrast, many re-\\nsearch studies aim to directly optimize the job’s performance\\nmetrics, rather than just the amount of allocated resources.\\nFor example, in Quasar [7] users specify performance con-\\nstraints, not limits, and the scheduler is responsible for meet-\\ning them. In public clouds, in which jobs are assigned to\\nVMs with strict limits, Paris [ 36] recommends a VM con-\\nfiguration given a representative task and its performance\\nmetrics. D2C [12] is a horizontal autoscaler that scales the\\nnumber of replicas in each layer of a multi-tier application by\\nlearning the performance parameters of each layer (modeled\\nwith queuing theory). The learning process is on-line – the\\napplication does not have to be benchmarked in advance.\\nEven more specific optimizations are possible if the cat-\\negory of jobs is more narrow. Ernest [33] focuses on batch,\\nmachine learning jobs, while CherryPick [1] also considers\\nanalytical jobs. While this paper concentrates on serving\\njobs, Autopilot is also used by 88% of batch jobs at Google\\n(measured by CPU consumption). [\\n19] uses reinforcement\\nlearning (RL) to drive horizontal scaling of serving jobs (with\\na utility function taking into account the number of repli-\\ncas, the throughput and any response time SLO violations).\\nAutopilot’s ML recommender borrows some ideas from RL,\\nsuch as choosing a model and a limit based on its historical\\nperformance.\\n8 Conclusions\\nAutoscaling is crucial for cloud efficiency, reliability, and\\ntoil reduction. Manually-set limits not only waste resources\\n(the average limits are too high) but also lead to frequent\\nlimit violations as load increases, or when new versions of\\nservices are rolled out.\\nAutopilot is a vertical and horizontal autoscaler used at\\nGoogle. By automatically increasing the precision of limit set-\\ntings, it has reduced resource waste and increased reliability:\\nout-of-memory errors are less common, less severe and affect\\nfewer jobs. Some of these gains were reachable with a simple\\ntime-weighted sliding window algorithm, but switching to\\na more sophisticated algorithm inspired by reinforcement\\nlearning enabled significant further gains.\\nAutopiloted jobs now represent over 48% of our fleet-wide\\nusage. Achieving such a high adoption rate took significant\\ndevelopment and trust-building measures to achieve, even'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='Autopiloted jobs now represent over 48% of our fleet-wide\\nusage. Achieving such a high adoption rate took significant\\ndevelopment and trust-building measures to achieve, even\\nin the presence of evident gains in reliability and efficiency.\\nHowever, we demonstrated that this extra work has paid off,\\nas users no longer have to resize their jobs manually, and\\nthe improved reliability results in fewer on-call alerts. We\\nstrongly commend this approach to other users of large scale\\ncompute clusters.\\nAcknowledgments\\nThe authors of this paper performed measurements and\\nwrote the paper, but many other people were critical to\\nAutopilot’s success. Those who contributed directly to the\\nproject include: Ben Appleton, Filip Balejko, Joachim Bar-\\ntosik, Arek Betkier, Jason Choy, Krzysztof Chrobak, Sła-\\nwomir Chyłek, Krzysztof Czaiński, Marcin Gawlik, Andrea\\nGesmundo, David Greenaway, Filip Grządkowski, Krzysztof\\nGrygiel, Michał Jabczyński, Sebastian Kaliszewski, Paulina\\nKania, Tomek Kulczyński, Sergey Melnychuk, Dmitri Nikulin,\\nRafal Pytko, Natalia Sakowska, Piotr Skowron, Paweł Stradom-\\nski, David Symonds, Jacek Szmigiel, Michał Szostek, Lee\\nWalsh, Peter Ward, Jamie Wilkinson, Przemysław Witek,\\nLijie Wong, Janek Wróbel, Tomasz Zielonka.\\nReferences\\n[1] O. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, and\\nM. Zhang. Cherrypick: adaptively unearthing the best cloud configura-\\ntions for big data analytics. In 14th USENIX Symposium on Networked\\nSystems Design and Implementation (NSDI’17), pages 469–482, 2017.\\n[2] D. Breitgand and A. Epstein. Improving consolidation of virtual ma-\\nchines with risk-aware bandwidth oversubscription in compute clouds.\\nIn IEEE INFOCOM, pages 2861–2865. IEEE, 2012.\\n[3] B. Burns, B. Grant, D. Oppenheimer, E. Brewer, and J. Wilkes. Borg,\\nOmega, and Kubernetes. Queue, 14(1):10, 2016.\\n[4] M. Carvalho, W. Cirne, F. Brasileiro, and J. Wilkes. Long-term SLOs for\\nreclaimed cloud computing resources. In ACM Symposium on Cloud\\nComputing (SoCC’14), pages 1–13. ACM, 2014.\\n[5] C. Curino, S. Krishnan, K. Karanasos, S. Rao, G. M. Fumarola, B. Huang,\\nK. Chaliparambil, A. Suresh, Y. Chen, S. Heddaya, et al. Hydra: a feder-\\nated resource manager for data-center scale analytics. In USENIX Sym-\\nposium on Networked Systems Design and Implementation (NSDI’19),'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\npages 177–192, 2019.\\n[6] J. Dean and S. Ghemawat. MapReduce: simplified data processing on\\nlarge clusters. In 6th Conference on Symposium on Operating Systems\\nDesign & Implementation (OSDI’04), pages 10–10, San Francisco, CA,\\n2004. USENIX Association.\\n[7] C. Delimitrou and C. Kozyrakis. Quasar: resource-efficient and QoS-\\naware cluster management. ACM SIGPLAN Notices , 49(4):127–144,\\n2014.\\n[8] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable\\nmachine learning. arXiv preprint arXiv:1702.08608, 2017.\\n[9] M. Duggan, R. Shaw, J. Duggan, E. Howley, and E. Barrett. A multitime-\\nsteps-ahead prediction approach for scheduling live migration in cloud\\ndata centers. Software: Practice and Experience, 49(4):617–639, 2019.\\n[10] A. Evangelidis, D. Parker, and R. Bahsoon. Performance modelling and\\nverification of cloud-based auto-scaling policies. Future Generation\\nComputer Systems, 87:629–638, 2018.\\n[11] G. Galante, L. C. E. De Bona, A. R. Mury, B. Schulze, and\\nR. da Rosa Righi. An analysis of public clouds elasticity in the ex-\\necution of scientific applications: a survey. Journal of Grid Computing,\\n14(2):193–216, 2016.\\n[12] A. Gandhi, P. Dube, A. Karve, A. Kochut, and L. Zhang. Adaptive,\\nmodel-driven autoscaling for cloud applications. In 11th International\\nConference on Autonomic Computing (ICAC’14), pages 57–64, 2014.\\n[13] Z. Gong, X. Gu, and J. Wilkes. Press: predictive elastic resource scaling\\nfor cloud systems. In International Conference on Network and Service\\nManagement, pages 9–16. IEEE, 2010.\\n[14] R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao, and A. Akella.\\nMulti-resource packing for cluster schedulers. In ACM SIGCOMM’14,\\npages 455–466. ACM, 2014.\\n[15] K. Grygiel and M. Wielgus. Kubernetes vertical pod autoscaler:\\ndesign proposal.https://github.com/kubernetes/community/blob/\\nmaster/contributors/design-proposals/autoscaling/vertical-pod-\\nautoscaler.md, kubernetes community, 2018. Accessed 2019-11-04.\\n[16] A. R. Hummaida, N. W. Paton, and R. Sakellariou. Adaptation in cloud\\nresource configuration: a survey. Journal of Cloud Computing, 5(1):7,\\n2016.\\n[17] A. Ilyushkin, A. Ali-Eldin, N. Herbst, A. V. Papadopoulos, B. Ghit,\\nD. Epema, and A. Iosup. An experimental performance evaluation\\nof autoscaling policies for complex workflows. In 8th ACM/SPEC\\non International Conference on Performance Engineering, pages 75–86.\\nACM, 2017.\\n[18] S. Islam, J. Keung, K. Lee, and A. Liu. Empirical prediction models\\nfor adaptive resource provisioning in the cloud.Future Generation\\nComputer Systems, 28(1):155–162, 2012.\\n[19] P. Jamshidi, A. M. Sharifloo, C. Pahl, A. Metzger, and G. Estrada. Self-\\nlearning cloud controllers: fuzzy q-learning for knowledge evolution.\\nIn International Conference on Cloud and Autonomic Computing, pages\\n208–211. IEEE, 2015.\\n[20] D. Janardhanan and E. Barrett. CPU workload forecasting of machines\\nin data centers using LSTM recurrent neural networks and ARIMA\\nmodels. In12th International Conference for Internet Technology and\\nSecured Transactions (ICITST’17), pages 55–60. IEEE, 2017.\\n[21] P. Janus and K. Rzadca. SLO-aware colocation of data center tasks\\nbased on instantaneous processor requirements. In ACM Symposium\\non Cloud Computing (SoCC’17), pages 256–268. ACM, 2017.\\n[22] T. Lorido-Botran, J. Miguel-Alonso, and J. A. Lozano. A review of\\nauto-scaling techniques for elastic applications in cloud environments.\\nJournal of Grid Computing, 12(4):559–592, 2014.\\n[23] C. Lu, K. Ye, G. Xu, C.-Z. Xu, and T. Bai. Imbalance in the cloud: an\\nanalysis on Alibaba cluster trace. In IEEE International Conference on\\nBig Data (BigData’17), pages 2884–2892. IEEE, 2017.\\n[24] V. Podolskiy, A. Jindal, and M. Gerndt. IaaS reactive autoscaling\\nperformance challenges. In 11th IEEE International Conference on\\nCloud Computing (CLOUD’18), pages 954–957. IEEE, 2018.\\n[25] G. Rattihalli, M. Govindaraju, H. Lu, and D. Tiwari. Exploring potential'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='performance challenges. In 11th IEEE International Conference on\\nCloud Computing (CLOUD’18), pages 954–957. IEEE, 2018.\\n[25] G. Rattihalli, M. Govindaraju, H. Lu, and D. Tiwari. Exploring potential\\nfor non-disruptive vertical auto scaling and resource estimation in\\nKubernetes. In 12th IEEE International Conference on Cloud Computing\\n(CLOUD’19), pages 33–40. IEEE, 2019.\\n[26] C. Reiss, A. Tumanov, G. R. Ganger, R. H. Katz, and M. A. Kozuch.\\nHeterogeneity and dynamicity of clouds at scale: Google trace analysis.\\nIn 3rd ACM Symposium on Cloud Computing (SoCC’12) , page 7. ACM,\\n2012.\\n[27] C. Reiss, J. Wilkes, and J. L. Hellerstein. Google cluster-usage traces: for-\\nmat + schema. Technical report at https://github.com/google/cluster-\\ndata/, Google, Mountain View, CA, USA, 2011.\\n[28] N. Roy, A. Dubey, and A. Gokhale. Efficient autoscaling in the cloud\\nusing predictive models for workload forecasting. In 4th IEEE Inter-\\nnational Conference on Cloud Computing (CLOUD’11), pages 500–507.\\nIEEE, 2011.\\n[29] M. Schwarzkopf, A. Konwinski, M. Abd-El-Malek, and J. Wilkes.\\nOmega: flexible, scalable schedulers for large compute clusters. In 8th\\nACM European Conference on Computer Systems (EuroSys’13), pages\\n351–364. ACM, 2013.\\n[30] X. Sun, C. Hu, R. Yang, P. Garraghan, T. Wo, J. Xu, J. Zhu, and C. Li.\\nROSE: cluster resource scheduling via speculative over-subscription.\\nIn 38th IEEE International Conference on Distributed Computing Systems\\n(ICDCS), pages 949–960. IEEE, 2018.\\n[31] M. Tirmazi, A. Barker, N. Deng, Z. G. Qin, M. E. Haque, S. Hand,\\nM. Harchol-Balter, and J. Wilkes. Borg: the Next Generation. In\\n15th ACM European Conference on Computer Systems (EuroSys’20),\\nHeraklion, Crete, Greece, 2020.\\n[32] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar,\\nR. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al. Apache Hadoop\\nYARN: Yet Another Resource Negotiator. In 4th ACM Symposium on\\nCloud Computing (SoCC’13), page 5. ACM, 2013.\\n[33] S. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica. Ernest:\\nEfficient performance prediction for large-scale advanced analytics.\\nIn 13th USENIX Symposium on Networked Systems Design and Imple-\\nmentation (NSDI’16), pages 363–378, 2016.\\n[34] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and\\nJ. Wilkes. Large-scale cluster management at Google with Borg. In\\n10th ACM European Conference on Computer Systems (EuroSys’15),\\npage 18. ACM, 2015.\\n[35] J. Wilkes. Google cluster usage traces v3. Technical report at https:\\n//github.com/google/cluster-data, Google, Mountain View, CA, USA,\\n2020.\\n[36] N. J. Yadwadkar, B. Hariharan, J. E. Gonzalez, B. Smith, and R. H.\\nKatz. Selecting the best VM across multiple public clouds: A data-\\ndriven performance modeling approach. In ACM Symposium on Cloud\\nComputing (SoCC’17), pages 452–465, 2017.\\n[37] A. B. Yoo, M. A. Jette, and M. Grondona. Slurm: simple Linux utility\\nfor resource management. In Workshop on Job Scheduling Strategies\\nfor Parallel Processing, pages 44–60. Springer, 2003.\\n[38] X. Zhang, E. Tune, R. Hagmann, R. Jnagal, V. Gokhale, and J. Wilkes.\\nCPI2: CPU performance isolation for shared compute clusters. In 8th\\nACM European Conference on Computer Systems (EuroSys’13), pages\\n379–391, 2013.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Split PDF into pages\n",
    "loader = PyPDFLoader(\"./resources/gke_autopilot_paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull embeding model from https://ollama.com/blog/embedding-models\n",
    "!ollama pull mxbai-embed-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "# Store the embeddings into memory for experiment purposes\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='are not Autopiloted (for these jobs Autopilot runs in simu-\\nlation mode). The dashboard helps the user to understand\\nAutopilot’s actions, and build trust in what Autopilot would\\ndo if it was enabled on non-Autopiloted jobs: the user can\\nsee how Autopilot would respond to daily and weekly cycles,\\nnew versions of binaries or suddenly changing loads.\\n5.3 Automatic migration\\nOnce we had sufficient trust in Autopilot’s actions from\\nlarge-scale off-line simulation studies and smaller-scale A/B\\nexperiments, we enabled it as a default for all existing small\\njobs (with an aggregate limit of up to roughly 10 machines)\\nand all new jobs. Users were notified well in advance and\\nthey were able to opt-out. This automatic migration trivially\\nincreased adoption with practically no user backlash.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\n3.1 Architecture\\nThe functional architecture of Autopilot is a triple of closed-\\nloop control systems, one for horizontal scaling at the per-job\\nlevel, the other two for vertical scaling of per-task resources\\n(CPU and memory). The algorithms (described in detail in\\nsubsequent sections) act as controllers. Autopilot considers\\njobs separately – there is no cross-job learning.\\nAutopilot’s implementation (Figure 1) takes the form of a\\ncollection of standard jobs on our infrastructure: each cluster\\nhas its own Autopilot. Each of Autopilot’s resource recom-\\nmenders (that size a job according to its historic usage) runs\\nas a separate job, with three task replicas for reliability. A\\nreplicatedAutopilot service, with an elected master, is re-\\nsponsible for selecting the recommender to use for a job and\\npassing (filtered) recommendations to the Borgmaster via an\\nactuator job. If the request is to change the number of tasks,\\nthe Borgmaster spawns or terminates tasks accordingly. If\\nthe request is to change resource limits, the Borgmaster\\nfirsts makes any scheduling decisions needed to accommo-\\ndate them, and then contacts the appropriate Borglet agents\\nto apply the changes. An independent monitoring system\\nkeeps track of how many resources each task uses; Autopilot\\njust subscribes to updates from it.\\nToday, our users explicitly opt-in their jobs to use Au-\\ntopilot, using a simple flag setting; we are in the process of\\nmaking this the default, and instead allowing explicit opt-\\nouts. Users may also configure several aspects of Autopilot’s\\nbehavior, such as: (1) forcing all replicas to have the same\\nlimit, which is useful for a failure tolerant program with just\\none active master; and (2) increasing the limits so that load\\nfrom a replica job in another cluster will be able to fail over\\nto this one instantaneously.\\nWhen a Autopiloted job is submitted to the Borgmaster,\\nit is temporarily queued until Autopilot has had a chance to\\nmake an initial resource recommendation for it. After that,\\nit proceeds through the normal scheduling processes.\\n3.2 Vertical (per task) autoscaling\\nThe Autopilot service chooses the recommender(s) to use for\\na job according to whether the resource being autoscaled is\\nmemory or CPU; how tolerant the job is to out-of-resource\\nevents (latency tolerant or not, OOM-sensitive vs. OOM-\\ntolerant); and optional user inputs, which can include an\\nexplicit recommender choice, or additional parameters to\\ncontrol Autopilot’s behavior, such as upper and lower bounds\\non the limits that can be set.\\n3.2.1 Preprocessing: aggregating the input signal. The\\nrecommenders use a preprocessed resource usage signal.\\nMost of this preprocessing is done by our monitoring system\\nto reduce the storage needs for historical data. The format\\nof aggregated signal is similar to the data provided in [35].\\nThe low-level task monitoring records a raw signal that\\nis a time series of measurements for each task of a job (e.g.,\\nTable 1. Notation used to describe the recommenders\\n𝑟𝑖 [𝜏] a\\nraw, per-task CPU/MEM time series (1s resolution)\\n𝑠𝑖 [𝑡] an aggregated, per-task CPU/MEM time series\\n(histograms, 5 min resolution)\\n𝑠[𝑡] an aggregated, per-job CPU/MEM time series\\n(histograms, 5 min resolution)\\nℎ[𝑡] a per-job load-adjusted histogram\\n𝑏[𝑘] the value of k-th bin (boundary value) of the histogram\\n𝑤[𝜏] the weight to decay the sample aged 𝜏\\n𝑆[𝑡] the moving window recommendation at time 𝑡\\n𝑚 a model (a parametrized arg minalgorithm)\\n𝑑𝑚 the decay rate used by model 𝑚\\n𝑀𝑚 the safety margin used by model 𝑚\\n𝐿 the value of a limit tested by the recommender\\n𝐿𝑚 [𝑡] the limit recommended by model 𝑚\\n𝐿[𝑡] the final recommendation of the ML recommender\\n𝑜(𝐿) the overrun cost of a limit 𝐿\\n𝑢(𝐿) the underrun cost of a limit 𝐿\\n𝑤𝑜 weight of the overrun cost\\n𝑤𝑢 weight of the underrun cost\\n𝑤Δ𝐿 weight of the penalty to change the limit\\n𝑤Δ𝑚 weight of the penalty to change the model'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='sensitivity.\\nFor memory, Autopilot uses different statistics as a func-\\ntion of the job’s OOM tolerance. This is set by default to\\n“low” for most large jobs, and “minimal” (the most stringent)\\nfor small ones, but can be overridden by the user:\\n•𝑆𝑝98 for jobs with low OOM tolerance.\\n•𝑆max for jobs with minimal OOM tolerance.\\n•For jobs with intermediate OOM tolerance, we select\\nvalues to (partially) cover short load spikes; we do this\\nby using the maximum of 𝑆𝑝60, the weighted 60%ile,\\nand 0.5𝑆max, half of the peak usage.\\nFinally, these raw recommendations are post-processed\\nbefore being applied. First, a recommendation is increased\\nby a 10–15% safety margin (less for large limits). Then, we\\ntake the maximum recommendation value seen over the last\\nhour to reduce fluctuations.\\n3.2.3 Recommenders based on machine learning. In\\nprinciple, Autopilot solves a machine learning problem: for\\na job, based on its past usage, find a limit that optimizes a\\nfunction expressing both the job’s and the infrastructure’s\\ngoals. The method described in the previous section (setting'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20240909001400', 'source': './resources/gke_autopilot_paper.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='EuroSys ’20, April 27–30, 2020, Heraklion, Greece Rzadca et. al\\na limit based on a simple statistics on a moving window)\\nspecifies an algorithm to solve this problem. In contrast,\\nAutopilot’s ML recommenders start with a cost function – a\\nspecification of the desired solution – and then for each job\\npick appropriate parameters of a model to optimize this cost\\nfunction. Such automation allows Autopilot to optimize for\\neach job the parameters that the previous method fixed for\\nall jobs, such as the decay rate 𝑡1/2, the safety margin or the\\ndownscaling stabilization period.\\nInternally, an ML recommender is composed of a large\\nnumber of models. For each job the recommender periodi-\\ncally chooses the best-performing model (according to the\\ncost function defined below computed over historical usage);\\nthen the chosen model is responsible for setting the limits.\\nEach model is a simple arg min-type algorithm minimizing\\nthe cost – models differ by weights assigned to individual\\nelements of\\narg min. One of the recurring problems of ML\\nmethods is interpretability of their results [8]: in Autopilot,\\nthe limits that the recommender sets must be explainable to\\nthe job owner. Having many simple models helps in inter-\\npreting ML recommender’s actions: a single model roughly\\ncorresponds to inferred characteristics of a job (e.g., models\\nwith long stabilization times correspond to jobs that have\\nrapidly changing utilizations). Then, given the weights im-\\nposed by the chosen model, its decisions are easy to interpret.\\nMore formally, for a signal 𝑠, at time 𝑡 the ML recom-\\nmender chooses from an ensemble of models {𝑚}a single\\nmodel 𝑚[𝑡]that is used to recommend the limits. A model\\nis a parameterized arg minalgorithm that computes a limit\\ngiven historical signal values. A model 𝑚is parameterized\\nby a decay rate 𝑑𝑚 and a safety margin 𝑀𝑚.\\nAt each time instant 𝑡, a model tests all possible limit val-\\nues 𝐿(possible limit values correspond to subsequent bounds\\nof the histogram buckets, 𝐿 ∈ {𝑏[0],...,𝑏 [𝑘]}). For each\\nlimit value 𝐿, the model computes the current costs of under-\\nand overruns based on the most recent usage histogram 𝑠[𝑡]\\nand then exponentially smooths it with the historic value.\\nThe overrun cost 𝑜(𝐿)[𝑡]counts the number of samples in\\nbuckets over the limit 𝐿in the most recent histogram:\\n𝑜(𝐿)[𝑡]= (1−𝑑𝑚)(𝑜(𝐿)[𝑡−1])+𝑑𝑚\\n\\x10Í\\n𝑗:𝑏 [𝑗 ]>𝐿 𝑠[𝑡][𝑗]\\n\\x11\\n. (4)\\nSimilarly, the underrun cost 𝑢(𝐿)[𝑡]counts the number of\\nsamples in buckets below the limit 𝐿,\\n𝑢(𝐿)[𝑡]= (1−𝑑𝑚)(𝑢(𝐿)[𝑡−1])+𝑑𝑚\\n\\x10Í\\n𝑗:𝑏 [𝑗 ]<𝐿 𝑠[𝑡][𝑗]\\n\\x11\\n. (5)\\nThen, a model picks a limit 𝐿′\\n𝑚 [𝑡]that minimizes a weighted\\nsum of overruns, underruns and a penalty Δ(𝐿,𝐿′\\n𝑚 [𝑡 −1])\\nfor a possible change of the limit:\\n𝐿′\\n𝑚 [𝑡]= arg min\\n𝐿\\n\\x10\\n𝑤𝑜𝑜(𝐿)[𝑡]+𝑤𝑢𝑢(𝐿)[𝑡]+𝑤Δ𝐿Δ(𝐿,𝐿′\\n𝑚 [𝑡−1])\\n\\x11\\n,\\n(6)\\nwhere Δ(𝑥,𝑦) = 1 if 𝑥 ≠ 𝑦 and 0 otherwise. (Using the\\nKronecker delta, Δ(𝑥,𝑦)= 1 −𝛿𝑥,𝑦 .) This function captures\\nthe three key costs of making resource allocation decisions\\nin a large-scale system. Overruns express the cost of the\\nlost opportunity – in a serving job when an overrun occurs\\nqueries get delayed, meaning some end-users might be less\\nwilling to continue using the system. Underruns express the\\ncost of the infrastructure: the more resources a job reserves,\\nthe more electricity, machines and people are needed. The\\npenalty termΔ helps avoid changing the limits too frequently,\\nbecause that can result in the task no longer fitting on its\\ncurrent machine causing it (or other tasks) to be evicted.\\nFinally, the limit is increased by the safety margin𝑀𝑚, i.e.,\\n𝐿𝑚 [𝑡]= 𝐿′\\n𝑚 [𝑡]+𝑀𝑚. (7)\\nTo pick a model at runtime (and therefore to optimize the\\ndecay rate 𝑑𝑚 and the safety margin 𝑀𝑚 for a particular\\njob), the ML recommender maintains for each model its\\n(exponentially smoothed) cost 𝑐𝑚 which is a weighted sum\\nof overruns, underruns and penalties for limit changes:\\n𝑐𝑚 [𝑡]= 𝑑\\n\\x10\\n𝑤𝑜𝑜𝑚 (𝐿𝑚 [𝑡],𝑡)+𝑤𝑢𝑢𝑚 (𝐿𝑚 [𝑡],𝑡)+\\n𝑤Δ𝐿Δ(𝐿𝑚 [𝑡],𝐿𝑚 [𝑡−1])\\n\\x11\\n+(1 −𝑑)𝑐𝑚 [𝑡−1].\\n(8)\\nAs historic costs are included in 𝑐𝑚 [𝑡−1], the underrun 𝑢𝑚')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the retriever which will return the top-k results in terms of the similarity\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"What is autopilot?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autopilot refers to a set of software tools and algorithms developed by Google that aim to optimize resource allocation in large-scale systems, such as cloud computing platforms. The goal of Autopilot is to automatically adjust the amount of resources (e.g., CPU, memory) allocated to running tasks or jobs, based on their historical performance and other factors.\n",
      "\n",
      "In the context of the provided text, Autopilot appears to be a component of Google's Cloud Platform that uses machine learning algorithms to optimize resource allocation and reduce waste. The Autopilot system is designed to automatically adjust the limits for histogram buckets (i.e., the bounds within which data points are stored) in order to minimize overruns and underruns, while also taking into account the cost of changing these limits.\n",
      "\n",
      "Autopilot can be seen as a form of \"autopilot\" in the sense that it automates the process of resource allocation and optimization, allowing users to focus on other tasks without having to manually manage their resources."
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Leverage retriever to fetch the context and pipe into the prompt template\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# This should answer the question based on the context\n",
    "# chain.invoke({\"question\": \"What is autopilot?\"})\n",
    "for chunk in chain.stream({\"question\": \"What is autopilot?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
